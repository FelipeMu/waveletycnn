{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias para trabajar con la red neuronal y procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # type: ignore\n",
    "from scipy.io import loadmat # type: ignore\n",
    "import tensorflow as tf # Para red neuronal profunda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time # Para tomar el tiempo de entrenamiento de la red\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################<br>\n",
    "##################### Convertir matrices de .mat a .npy ##########################<br>\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directorios de entrada y salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pam_dir = 'D:/TT/Memoria/waveletycnn/codigo_matlab/codigo_fuente/matrices_complejas_pam_mat' # matrices en formato .mat\n",
    "input_vsc_dir = 'D:/TT/Memoria/waveletycnn/codigo_matlab/codigo_fuente/matrices_complejas_vsc_mat' # matrices en formato .mat\n",
    "output_pam_dir = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_pam_python' # INPUT PARA LA RED\n",
    "output_vsc_dir = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_vsc_python' # OUTPUT O SALIDAS ESPERADAS PARA LA RED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear los directorios de salida si no existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_pam_dir, exist_ok=True)\n",
    "os.makedirs(output_vsc_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para convertir archivos .mat a .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mat_to_npy(input_dir, output_dir, prefix):\n",
    "    for i in range(1, 31):\n",
    "        mat_file = os.path.join(input_dir, f'{prefix}_noise_{i}.mat')\n",
    "        npy_file = os.path.join(output_dir, f'{prefix}_noise_{i}.npy')\n",
    "        \n",
    "        # Cargar el archivo .mat\n",
    "        mat_data = loadmat(mat_file)\n",
    "        \n",
    "        # Extraer la matriz compleja\n",
    "        matrix_key = [key for key in mat_data.keys() if not key.startswith('__')][0]\n",
    "        matrix = mat_data[matrix_key]\n",
    "        \n",
    "        # Guardar la matriz en formato .npy\n",
    "        np.save(npy_file, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir archivos .mat a .npy para PAM y VSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_mat_to_npy(input_pam_dir, output_pam_dir, 'matrix_complex_pam')\n",
    "convert_mat_to_npy(input_vsc_dir, output_vsc_dir, 'matrix_complex_vsc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################<br>\n",
    "##################### Conversion a tensor tridimensional ##########################<br>\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directorios de entrada y salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pam_dir = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_pam_python'\n",
    "input_vsc_dir = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_vsc_python'\n",
    "output_pam_dir = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_pam_procesadas'\n",
    "output_vsc_dir = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_vsc_procesadas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear directorios de salida si no existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_pam_dir, exist_ok=True)\n",
    "os.makedirs(output_vsc_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para procesar las matrices complejas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_matriz_compleja(matriz_compleja):\n",
    "    datos_organizados = np.stack((matriz_compleja.real, matriz_compleja.imag), axis=-1)\n",
    "    return datos_organizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesar matrices complejas en la carpeta input_pam_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_pam_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        input_path = os.path.join(input_pam_dir, filename)\n",
    "        output_path = os.path.join(output_pam_dir, filename)\n",
    "        \n",
    "        # Cargar la matriz compleja\n",
    "        matriz_compleja = np.load(input_path)\n",
    "        \n",
    "        # Procesar la matriz compleja\n",
    "        datos_organizados = procesar_matriz_compleja(matriz_compleja)\n",
    "        \n",
    "        # Guardar los datos procesados\n",
    "        np.save(output_path, datos_organizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesar matrices complejas en la carpeta input_vsc_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completado.\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(input_vsc_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        input_path = os.path.join(input_vsc_dir, filename)\n",
    "        output_path = os.path.join(output_vsc_dir, filename)\n",
    "        \n",
    "        # Cargar la matriz compleja\n",
    "        matriz_compleja = np.load(input_path)\n",
    "        \n",
    "        # Procesar la matriz compleja\n",
    "        datos_organizados = procesar_matriz_compleja(matriz_compleja)\n",
    "        \n",
    "        # Guardar los datos procesados\n",
    "        np.save(output_path, datos_organizados)\n",
    "print(\"Procesamiento completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################<br>\n",
    "################## Verificacicon de \"shape\" - matrices pam y vsc ##################<br>\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directorios de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pam_dir_check = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_pam_procesadas'\n",
    "output_vsc_dir_check = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_vsc_procesadas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para verificar la forma de una matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_shape(directorio, nombre_archivo):\n",
    "    path = os.path.join(directorio, nombre_archivo)\n",
    "    matriz = np.load(path)\n",
    "    return matriz.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar la forma de un archivo de ejemplo en output_pam_dir_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de matrix_complex_pam_noise_1.npy en D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_pam_procesadas: (36, 1024, 2)\n"
     ]
    }
   ],
   "source": [
    "ejemplo_pam = os.listdir(output_pam_dir_check)[0]  # Obtener el primer archivo de la carpeta\n",
    "shape_pam = verificar_shape(output_pam_dir_check, ejemplo_pam)\n",
    "print(f\"Shape de {ejemplo_pam} en {output_pam_dir_check}: {shape_pam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar la forma de un archivo de ejemplo en output_vsc_dir_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de matrix_complex_vsc_noise_1.npy en D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_vsc_procesadas: (36, 1024, 2)\n"
     ]
    }
   ],
   "source": [
    "ejemplo_vsc = os.listdir(output_vsc_dir_check)[0]  # Obtener el primer archivo de la carpeta\n",
    "shape_vsc = verificar_shape(output_vsc_dir_check, ejemplo_vsc)\n",
    "print(f\"Shape de {ejemplo_vsc} en {output_vsc_dir_check}: {shape_vsc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################<br>\n",
    "####################### Red Neuronal Profunda: U-net #############################<br>\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directorios de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pam_dir = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_pam_procesadas'\n",
    "output_vsc_dir = 'D:/TT/Memoria/waveletycnn/codigo_python/matrices_complejas_vsc_procesadas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para cargar los archivos .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy_files(input_dir):\n",
    "    files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.npy')])\n",
    "    data = [np.load(f) for f in files]\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar los datos de entrada y salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_npy_files(input_pam_dir) # inputs\n",
    "Y = load_npy_files(output_vsc_dir) # outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar las formas de los datos cargados (# entradas, filas, columnas, canales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de los inputs (X): (30, 36, 1024, 2)\n",
      "Shape de los outputs (Y): (30, 36, 1024, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape de los inputs (X): {X.shape}\")\n",
    "print(f\"Shape de los outputs (Y): {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir la U-Net con regularizacion L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def unet_model_with_l2(input_shape, l2_lambda):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Regularizer\n",
    "    l2_reg = tf.keras.regularizers.l2(l2_lambda)\n",
    "    \n",
    "    # Encoder\n",
    "    c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(inputs)\n",
    "    c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(p1)\n",
    "    c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(p2)\n",
    "    c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(c3)\n",
    "    \n",
    "    # Decoder\n",
    "    u4 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c3)\n",
    "    u4 = tf.keras.layers.concatenate([u4, c2])\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(u4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(c4)\n",
    "    \n",
    "    u5 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u5 = tf.keras.layers.concatenate([u5, c1])\n",
    "    c5 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(u5)\n",
    "    c5 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2_reg)(c5)\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(2, (1, 1), activation='linear')(c5)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model\n",
    "'''\n",
    "\n",
    "def unet_model_with_l2(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Regularizer\n",
    "    #l2_reg = tf.keras.regularizers.l2(l2_lambda)\n",
    "    \n",
    "    # Encoder\n",
    "    c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs) #filtro original=64\n",
    "    c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1) #filtro original=64\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1) #filtro original=128\n",
    "    c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2) #filtro original=128\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2) #filtro original=256\n",
    "    c3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3) #filtro original=256\n",
    "    \n",
    "    # Decoder\n",
    "    u4 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c3) #filtro original=128\n",
    "    u4 = tf.keras.layers.concatenate([u4, c2])\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u4) #filtro original=128\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c4) #filtro original=128\n",
    "    \n",
    "    u5 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4) #filtro original=64\n",
    "    u5 = tf.keras.layers.concatenate([u5, c1])\n",
    "    c5 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u5) #filtro original=64\n",
    "    c5 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c5) #filtro original=64\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(2, (1, 1), activation='linear')(c5)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir la metrica NMSE ajustada para utilizar la varianza de los valores verdaderos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmse(y_true, y_pred):\n",
    "    mse = tf.keras.backend.mean(tf.keras.backend.square(y_true - y_pred))\n",
    "    var_true = tf.keras.backend.var(y_true)\n",
    "    return mse / var_true"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "**HIPERPARAMETROS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 700\n",
    "batchsize = 8\n",
    "learning_rate = 0.0001\n",
    "#l2_lambda = 0.01\n",
    "validation_split = 0.3 # 70% entrenamiento & 30% validacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[1:]  # forma del input a entrar. en este caso esta forma debe coincidir con las matrices que entran a la red\n",
    "#model = unet_model_with_l2(input_shape, l2_lambda)\n",
    "model = unet_model_with_l2(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINICION DE LA FUNCION DE DECAIMIENTO, ALGORITMO OPTIMIZADOR, FUNCION DE PERDIDA Y METRICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing-testing-testing-testing-testing-testing-testing\n",
    "\n",
    "\n",
    "# This function keeps the learning rate at 0.001 for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def scheduler(epoch , lr):\n",
    "  if epoch < 100:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(0.1 * (-0.1))\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[nmse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################################################################<br>\n",
    "#########################################################################################################<br>\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENTRENAMIENTO DE LA RED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21 samples, validate on 9 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 1/700\n",
      "21/21 [==============================] - 9s 412ms/sample - loss: 0.3775 - nmse: 0.9663 - val_loss: 0.3443 - val_nmse: 0.9251\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 2/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.3564 - nmse: 0.9130 - val_loss: 0.3268 - val_nmse: 0.8758\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 3/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.3398 - nmse: 0.8708 - val_loss: 0.3142 - val_nmse: 0.8396\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 4/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.3290 - nmse: 0.8446 - val_loss: 0.3080 - val_nmse: 0.8203\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 5/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.3239 - nmse: 0.8281 - val_loss: 0.3051 - val_nmse: 0.8107\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 6/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.3208 - nmse: 0.8245 - val_loss: 0.3003 - val_nmse: 0.7974\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 7/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.3156 - nmse: 0.8067 - val_loss: 0.2944 - val_nmse: 0.7819\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 8/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.3096 - nmse: 0.7943 - val_loss: 0.2884 - val_nmse: 0.7655\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 9/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.3032 - nmse: 0.7736 - val_loss: 0.2808 - val_nmse: 0.7440\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 10/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.2951 - nmse: 0.7533 - val_loss: 0.2715 - val_nmse: 0.7174\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 11/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.2862 - nmse: 0.7325 - val_loss: 0.2629 - val_nmse: 0.6927\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 12/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2778 - nmse: 0.7104 - val_loss: 0.2550 - val_nmse: 0.6703\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 13/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2701 - nmse: 0.6892 - val_loss: 0.2475 - val_nmse: 0.6490\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 14/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.2627 - nmse: 0.6754 - val_loss: 0.2407 - val_nmse: 0.6294\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 15/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2567 - nmse: 0.6595 - val_loss: 0.2329 - val_nmse: 0.6077\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 16/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2497 - nmse: 0.6461 - val_loss: 0.2311 - val_nmse: 0.6019\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 17/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2469 - nmse: 0.6367 - val_loss: 0.2220 - val_nmse: 0.5778\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 18/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.2395 - nmse: 0.6171 - val_loss: 0.2181 - val_nmse: 0.5672\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 19/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2349 - nmse: 0.6050 - val_loss: 0.2148 - val_nmse: 0.5577\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 20/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2301 - nmse: 0.5857 - val_loss: 0.2106 - val_nmse: 0.5461\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 21/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2262 - nmse: 0.5788 - val_loss: 0.2043 - val_nmse: 0.5291\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 22/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.2220 - nmse: 0.5644 - val_loss: 0.2010 - val_nmse: 0.5197\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 23/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2168 - nmse: 0.5563 - val_loss: 0.1981 - val_nmse: 0.5112\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 24/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.2161 - nmse: 0.5546 - val_loss: 0.1959 - val_nmse: 0.5048\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 25/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.2106 - nmse: 0.5360 - val_loss: 0.1918 - val_nmse: 0.4938\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 26/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.2077 - nmse: 0.5314 - val_loss: 0.1864 - val_nmse: 0.4788\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 27/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.2028 - nmse: 0.5250 - val_loss: 0.1816 - val_nmse: 0.4654\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 28/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1980 - nmse: 0.5136 - val_loss: 0.1793 - val_nmse: 0.4593\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 29/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1960 - nmse: 0.5080 - val_loss: 0.1781 - val_nmse: 0.4556\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 30/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1958 - nmse: 0.4918 - val_loss: 0.1718 - val_nmse: 0.4386\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 31/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1904 - nmse: 0.4921 - val_loss: 0.1756 - val_nmse: 0.4477\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 32/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1930 - nmse: 0.4941 - val_loss: 0.1672 - val_nmse: 0.4251\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 33/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1846 - nmse: 0.4683 - val_loss: 0.1631 - val_nmse: 0.4143\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 34/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1811 - nmse: 0.4660 - val_loss: 0.1611 - val_nmse: 0.4082\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 35/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1782 - nmse: 0.4607 - val_loss: 0.1584 - val_nmse: 0.4008\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 36/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1749 - nmse: 0.4476 - val_loss: 0.1556 - val_nmse: 0.3930\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 37/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1726 - nmse: 0.4439 - val_loss: 0.1542 - val_nmse: 0.3882\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 38/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1755 - nmse: 0.4502 - val_loss: 0.1718 - val_nmse: 0.4344\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 39/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1834 - nmse: 0.4693 - val_loss: 0.1598 - val_nmse: 0.4031\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 40/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1734 - nmse: 0.4501 - val_loss: 0.1546 - val_nmse: 0.3892\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 41/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1696 - nmse: 0.4312 - val_loss: 0.1470 - val_nmse: 0.3689\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 42/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1653 - nmse: 0.4187 - val_loss: 0.1499 - val_nmse: 0.3762\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 43/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1639 - nmse: 0.4133 - val_loss: 0.1433 - val_nmse: 0.3581\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 44/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1604 - nmse: 0.4118 - val_loss: 0.1413 - val_nmse: 0.3521\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 45/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1580 - nmse: 0.4024 - val_loss: 0.1403 - val_nmse: 0.3489\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 46/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1566 - nmse: 0.3978 - val_loss: 0.1388 - val_nmse: 0.3442\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 47/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1539 - nmse: 0.3943 - val_loss: 0.1368 - val_nmse: 0.3386\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 48/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1532 - nmse: 0.3855 - val_loss: 0.1365 - val_nmse: 0.3383\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 49/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1561 - nmse: 0.3952 - val_loss: 0.1372 - val_nmse: 0.3392\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 50/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1505 - nmse: 0.3919 - val_loss: 0.1378 - val_nmse: 0.3402\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 51/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1539 - nmse: 0.3857 - val_loss: 0.1330 - val_nmse: 0.3284\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 52/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1507 - nmse: 0.3891 - val_loss: 0.1321 - val_nmse: 0.3255\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 53/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1484 - nmse: 0.3758 - val_loss: 0.1306 - val_nmse: 0.3209\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 54/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1463 - nmse: 0.3710 - val_loss: 0.1305 - val_nmse: 0.3209\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 55/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1449 - nmse: 0.3729 - val_loss: 0.1300 - val_nmse: 0.3190\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 56/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1436 - nmse: 0.3705 - val_loss: 0.1291 - val_nmse: 0.3164\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 57/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1426 - nmse: 0.3593 - val_loss: 0.1266 - val_nmse: 0.3095\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 58/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1403 - nmse: 0.3605 - val_loss: 0.1245 - val_nmse: 0.3039\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 59/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1389 - nmse: 0.3586 - val_loss: 0.1239 - val_nmse: 0.3020\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 60/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1382 - nmse: 0.3563 - val_loss: 0.1227 - val_nmse: 0.2985\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 61/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1394 - nmse: 0.3577 - val_loss: 0.1256 - val_nmse: 0.3063\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 62/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1452 - nmse: 0.3696 - val_loss: 0.1227 - val_nmse: 0.2991\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 63/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1373 - nmse: 0.3523 - val_loss: 0.1224 - val_nmse: 0.2981\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 64/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1369 - nmse: 0.3441 - val_loss: 0.1188 - val_nmse: 0.2884\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 65/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1345 - nmse: 0.3414 - val_loss: 0.1170 - val_nmse: 0.2837\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 66/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1334 - nmse: 0.3465 - val_loss: 0.1169 - val_nmse: 0.2828\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 67/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1319 - nmse: 0.3388 - val_loss: 0.1166 - val_nmse: 0.2819\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 68/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1309 - nmse: 0.3403 - val_loss: 0.1153 - val_nmse: 0.2786\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 69/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1298 - nmse: 0.3365 - val_loss: 0.1149 - val_nmse: 0.2772\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 70/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1292 - nmse: 0.3273 - val_loss: 0.1136 - val_nmse: 0.2737\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 71/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1288 - nmse: 0.3239 - val_loss: 0.1137 - val_nmse: 0.2736\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 72/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1277 - nmse: 0.3334 - val_loss: 0.1165 - val_nmse: 0.2810\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 73/700\n",
      "21/21 [==============================] - 1s 51ms/sample - loss: 0.1333 - nmse: 0.3407 - val_loss: 0.1173 - val_nmse: 0.2835\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 74/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1304 - nmse: 0.3361 - val_loss: 0.1151 - val_nmse: 0.2778\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 75/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1284 - nmse: 0.3233 - val_loss: 0.1109 - val_nmse: 0.2665\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 76/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1262 - nmse: 0.3268 - val_loss: 0.1099 - val_nmse: 0.2635\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 77/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1246 - nmse: 0.3200 - val_loss: 0.1083 - val_nmse: 0.2593\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 78/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1233 - nmse: 0.3184 - val_loss: 0.1095 - val_nmse: 0.2623\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 79/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1241 - nmse: 0.3175 - val_loss: 0.1101 - val_nmse: 0.2640\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 80/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1249 - nmse: 0.3103 - val_loss: 0.1094 - val_nmse: 0.2620\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 81/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1215 - nmse: 0.3090 - val_loss: 0.1072 - val_nmse: 0.2560\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 82/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1232 - nmse: 0.3178 - val_loss: 0.1123 - val_nmse: 0.2695\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 83/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1244 - nmse: 0.3125 - val_loss: 0.1048 - val_nmse: 0.2499\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 84/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1200 - nmse: 0.3054 - val_loss: 0.1071 - val_nmse: 0.2556\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 85/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1193 - nmse: 0.3015 - val_loss: 0.1057 - val_nmse: 0.2519\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 86/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1196 - nmse: 0.2975 - val_loss: 0.1033 - val_nmse: 0.2455\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 87/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1187 - nmse: 0.3026 - val_loss: 0.1079 - val_nmse: 0.2573\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 88/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1189 - nmse: 0.3077 - val_loss: 0.1091 - val_nmse: 0.2608\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 89/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1292 - nmse: 0.3339 - val_loss: 0.1368 - val_nmse: 0.3343\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 90/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1336 - nmse: 0.3382 - val_loss: 0.1136 - val_nmse: 0.2731\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 91/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1237 - nmse: 0.3156 - val_loss: 0.1042 - val_nmse: 0.2487\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 92/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1201 - nmse: 0.3002 - val_loss: 0.1064 - val_nmse: 0.2544\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 93/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1199 - nmse: 0.3072 - val_loss: 0.1068 - val_nmse: 0.2548\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 94/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1190 - nmse: 0.3017 - val_loss: 0.1026 - val_nmse: 0.2437\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 95/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1165 - nmse: 0.3026 - val_loss: 0.1023 - val_nmse: 0.2428\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 96/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1153 - nmse: 0.2921 - val_loss: 0.1013 - val_nmse: 0.2403\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 97/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1142 - nmse: 0.2922 - val_loss: 0.0999 - val_nmse: 0.2362\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 98/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1128 - nmse: 0.2893 - val_loss: 0.1002 - val_nmse: 0.2368\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 99/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1123 - nmse: 0.2887 - val_loss: 0.0990 - val_nmse: 0.2336\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 9.999999747378752e-05.\n",
      "Epoch 100/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1117 - nmse: 0.2871 - val_loss: 0.0986 - val_nmse: 0.2326\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(9.9004974e-05, shape=(), dtype=float32).\n",
      "Epoch 101/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1108 - nmse: 0.2807 - val_loss: 0.0977 - val_nmse: 0.2302\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(9.801985e-05, shape=(), dtype=float32).\n",
      "Epoch 102/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1101 - nmse: 0.2838 - val_loss: 0.0978 - val_nmse: 0.2303\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(9.704453e-05, shape=(), dtype=float32).\n",
      "Epoch 103/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1099 - nmse: 0.2802 - val_loss: 0.0979 - val_nmse: 0.2309\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(9.607892e-05, shape=(), dtype=float32).\n",
      "Epoch 104/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1093 - nmse: 0.2803 - val_loss: 0.0970 - val_nmse: 0.2284\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(9.512291e-05, shape=(), dtype=float32).\n",
      "Epoch 105/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1088 - nmse: 0.2710 - val_loss: 0.0970 - val_nmse: 0.2283\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(9.4176416e-05, shape=(), dtype=float32).\n",
      "Epoch 106/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1084 - nmse: 0.2824 - val_loss: 0.0955 - val_nmse: 0.2243\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(9.323934e-05, shape=(), dtype=float32).\n",
      "Epoch 107/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1074 - nmse: 0.2664 - val_loss: 0.0953 - val_nmse: 0.2238\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(9.231159e-05, shape=(), dtype=float32).\n",
      "Epoch 108/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1068 - nmse: 0.2740 - val_loss: 0.0951 - val_nmse: 0.2231\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(9.139306e-05, shape=(), dtype=float32).\n",
      "Epoch 109/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1063 - nmse: 0.2713 - val_loss: 0.0947 - val_nmse: 0.2221\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(9.0483685e-05, shape=(), dtype=float32).\n",
      "Epoch 110/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1062 - nmse: 0.2732 - val_loss: 0.0962 - val_nmse: 0.2259\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(8.958335e-05, shape=(), dtype=float32).\n",
      "Epoch 111/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1107 - nmse: 0.2874 - val_loss: 0.1123 - val_nmse: 0.2682\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(8.869197e-05, shape=(), dtype=float32).\n",
      "Epoch 112/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1266 - nmse: 0.3218 - val_loss: 0.0939 - val_nmse: 0.2205\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(8.7809465e-05, shape=(), dtype=float32).\n",
      "Epoch 113/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1100 - nmse: 0.2791 - val_loss: 0.0946 - val_nmse: 0.2224\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(8.693574e-05, shape=(), dtype=float32).\n",
      "Epoch 114/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1086 - nmse: 0.2817 - val_loss: 0.0964 - val_nmse: 0.2268\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(8.607071e-05, shape=(), dtype=float32).\n",
      "Epoch 115/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1074 - nmse: 0.2766 - val_loss: 0.0962 - val_nmse: 0.2264\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(8.5214284e-05, shape=(), dtype=float32).\n",
      "Epoch 116/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1066 - nmse: 0.2747 - val_loss: 0.0931 - val_nmse: 0.2180\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(8.436638e-05, shape=(), dtype=float32).\n",
      "Epoch 117/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1049 - nmse: 0.2621 - val_loss: 0.0933 - val_nmse: 0.2184\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(8.352692e-05, shape=(), dtype=float32).\n",
      "Epoch 118/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1045 - nmse: 0.2620 - val_loss: 0.0934 - val_nmse: 0.2185\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(8.2695806e-05, shape=(), dtype=float32).\n",
      "Epoch 119/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1037 - nmse: 0.2728 - val_loss: 0.0937 - val_nmse: 0.2192\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(8.187297e-05, shape=(), dtype=float32).\n",
      "Epoch 120/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1035 - nmse: 0.2664 - val_loss: 0.0927 - val_nmse: 0.2165\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(8.1058315e-05, shape=(), dtype=float32).\n",
      "Epoch 121/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1028 - nmse: 0.2602 - val_loss: 0.0916 - val_nmse: 0.2136\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(8.025177e-05, shape=(), dtype=float32).\n",
      "Epoch 122/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1023 - nmse: 0.2639 - val_loss: 0.0912 - val_nmse: 0.2124\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(7.9453246e-05, shape=(), dtype=float32).\n",
      "Epoch 123/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1017 - nmse: 0.2604 - val_loss: 0.0907 - val_nmse: 0.2114\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(7.866267e-05, shape=(), dtype=float32).\n",
      "Epoch 124/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1013 - nmse: 0.2614 - val_loss: 0.0905 - val_nmse: 0.2107\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(7.787996e-05, shape=(), dtype=float32).\n",
      "Epoch 125/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1006 - nmse: 0.2638 - val_loss: 0.0902 - val_nmse: 0.2097\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(7.710503e-05, shape=(), dtype=float32).\n",
      "Epoch 126/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.1003 - nmse: 0.2566 - val_loss: 0.0903 - val_nmse: 0.2102\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(7.633782e-05, shape=(), dtype=float32).\n",
      "Epoch 127/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.1003 - nmse: 0.2578 - val_loss: 0.0895 - val_nmse: 0.2083\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(7.557824e-05, shape=(), dtype=float32).\n",
      "Epoch 128/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0994 - nmse: 0.2556 - val_loss: 0.0897 - val_nmse: 0.2083\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(7.482622e-05, shape=(), dtype=float32).\n",
      "Epoch 129/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0992 - nmse: 0.2578 - val_loss: 0.0893 - val_nmse: 0.2072\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(7.408168e-05, shape=(), dtype=float32).\n",
      "Epoch 130/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0989 - nmse: 0.2451 - val_loss: 0.0888 - val_nmse: 0.2064\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(7.3344556e-05, shape=(), dtype=float32).\n",
      "Epoch 131/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0984 - nmse: 0.2488 - val_loss: 0.0889 - val_nmse: 0.2060\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(7.261476e-05, shape=(), dtype=float32).\n",
      "Epoch 132/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0983 - nmse: 0.2463 - val_loss: 0.0884 - val_nmse: 0.2048\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(7.189223e-05, shape=(), dtype=float32).\n",
      "Epoch 133/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0979 - nmse: 0.2517 - val_loss: 0.0896 - val_nmse: 0.2083\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(7.117689e-05, shape=(), dtype=float32).\n",
      "Epoch 134/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0990 - nmse: 0.2519 - val_loss: 0.0887 - val_nmse: 0.2057\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(7.046866e-05, shape=(), dtype=float32).\n",
      "Epoch 135/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0974 - nmse: 0.2561 - val_loss: 0.0881 - val_nmse: 0.2040\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(6.9767484e-05, shape=(), dtype=float32).\n",
      "Epoch 136/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0973 - nmse: 0.2490 - val_loss: 0.0875 - val_nmse: 0.2026\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(6.9073285e-05, shape=(), dtype=float32).\n",
      "Epoch 137/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0965 - nmse: 0.2420 - val_loss: 0.0872 - val_nmse: 0.2017\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(6.838599e-05, shape=(), dtype=float32).\n",
      "Epoch 138/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0960 - nmse: 0.2512 - val_loss: 0.0870 - val_nmse: 0.2008\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(6.7705536e-05, shape=(), dtype=float32).\n",
      "Epoch 139/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0959 - nmse: 0.2401 - val_loss: 0.0869 - val_nmse: 0.2009\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(6.703185e-05, shape=(), dtype=float32).\n",
      "Epoch 140/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0959 - nmse: 0.2421 - val_loss: 0.0867 - val_nmse: 0.2003\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(6.636487e-05, shape=(), dtype=float32).\n",
      "Epoch 141/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0952 - nmse: 0.2381 - val_loss: 0.0863 - val_nmse: 0.1990\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(6.5704524e-05, shape=(), dtype=float32).\n",
      "Epoch 142/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0948 - nmse: 0.2495 - val_loss: 0.0866 - val_nmse: 0.1999\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(6.505075e-05, shape=(), dtype=float32).\n",
      "Epoch 143/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0949 - nmse: 0.2491 - val_loss: 0.0864 - val_nmse: 0.1993\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(6.440348e-05, shape=(), dtype=float32).\n",
      "Epoch 144/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0949 - nmse: 0.2476 - val_loss: 0.0858 - val_nmse: 0.1978\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(6.376265e-05, shape=(), dtype=float32).\n",
      "Epoch 145/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0943 - nmse: 0.2380 - val_loss: 0.0857 - val_nmse: 0.1977\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(6.31282e-05, shape=(), dtype=float32).\n",
      "Epoch 146/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0937 - nmse: 0.2408 - val_loss: 0.0859 - val_nmse: 0.1980\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(6.250006e-05, shape=(), dtype=float32).\n",
      "Epoch 147/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0939 - nmse: 0.2418 - val_loss: 0.0862 - val_nmse: 0.1989\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(6.187817e-05, shape=(), dtype=float32).\n",
      "Epoch 148/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0945 - nmse: 0.2439 - val_loss: 0.0858 - val_nmse: 0.1978\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(6.126247e-05, shape=(), dtype=float32).\n",
      "Epoch 149/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0933 - nmse: 0.2353 - val_loss: 0.0850 - val_nmse: 0.1955\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(6.0652896e-05, shape=(), dtype=float32).\n",
      "Epoch 150/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0927 - nmse: 0.2424 - val_loss: 0.0860 - val_nmse: 0.1982\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(6.0049388e-05, shape=(), dtype=float32).\n",
      "Epoch 151/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0942 - nmse: 0.2390 - val_loss: 0.0851 - val_nmse: 0.1959\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(5.9451882e-05, shape=(), dtype=float32).\n",
      "Epoch 152/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0926 - nmse: 0.2373 - val_loss: 0.0846 - val_nmse: 0.1944\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(5.886032e-05, shape=(), dtype=float32).\n",
      "Epoch 153/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0924 - nmse: 0.2372 - val_loss: 0.0851 - val_nmse: 0.1957\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(5.8274647e-05, shape=(), dtype=float32).\n",
      "Epoch 154/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0921 - nmse: 0.2354 - val_loss: 0.0843 - val_nmse: 0.1939\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(5.76948e-05, shape=(), dtype=float32).\n",
      "Epoch 155/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0919 - nmse: 0.2395 - val_loss: 0.0839 - val_nmse: 0.1923\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(5.7120724e-05, shape=(), dtype=float32).\n",
      "Epoch 156/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0913 - nmse: 0.2253 - val_loss: 0.0839 - val_nmse: 0.1924\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(5.655236e-05, shape=(), dtype=float32).\n",
      "Epoch 157/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0911 - nmse: 0.2386 - val_loss: 0.0840 - val_nmse: 0.1928\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(5.598965e-05, shape=(), dtype=float32).\n",
      "Epoch 158/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0907 - nmse: 0.2391 - val_loss: 0.0838 - val_nmse: 0.1923\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(5.543254e-05, shape=(), dtype=float32).\n",
      "Epoch 159/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0909 - nmse: 0.2282 - val_loss: 0.0833 - val_nmse: 0.1908\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(5.4880977e-05, shape=(), dtype=float32).\n",
      "Epoch 160/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0902 - nmse: 0.2328 - val_loss: 0.0834 - val_nmse: 0.1911\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(5.4334898e-05, shape=(), dtype=float32).\n",
      "Epoch 161/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0899 - nmse: 0.2308 - val_loss: 0.0832 - val_nmse: 0.1906\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(5.3794254e-05, shape=(), dtype=float32).\n",
      "Epoch 162/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0899 - nmse: 0.2216 - val_loss: 0.0830 - val_nmse: 0.1900\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(5.325899e-05, shape=(), dtype=float32).\n",
      "Epoch 163/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0897 - nmse: 0.2345 - val_loss: 0.0828 - val_nmse: 0.1896\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(5.2729054e-05, shape=(), dtype=float32).\n",
      "Epoch 164/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0896 - nmse: 0.2330 - val_loss: 0.0831 - val_nmse: 0.1902\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(5.220439e-05, shape=(), dtype=float32).\n",
      "Epoch 165/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0893 - nmse: 0.2284 - val_loss: 0.0824 - val_nmse: 0.1884\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(5.1684943e-05, shape=(), dtype=float32).\n",
      "Epoch 166/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0890 - nmse: 0.2227 - val_loss: 0.0824 - val_nmse: 0.1883\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(5.1170668e-05, shape=(), dtype=float32).\n",
      "Epoch 167/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0886 - nmse: 0.2280 - val_loss: 0.0823 - val_nmse: 0.1879\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(5.0661507e-05, shape=(), dtype=float32).\n",
      "Epoch 168/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0886 - nmse: 0.2258 - val_loss: 0.0821 - val_nmse: 0.1875\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(5.0157414e-05, shape=(), dtype=float32).\n",
      "Epoch 169/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0881 - nmse: 0.2267 - val_loss: 0.0819 - val_nmse: 0.1872\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(4.9658338e-05, shape=(), dtype=float32).\n",
      "Epoch 170/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0881 - nmse: 0.2268 - val_loss: 0.0818 - val_nmse: 0.1864\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(4.9164228e-05, shape=(), dtype=float32).\n",
      "Epoch 171/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0877 - nmse: 0.2263 - val_loss: 0.0819 - val_nmse: 0.1868\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(4.8675032e-05, shape=(), dtype=float32).\n",
      "Epoch 172/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0881 - nmse: 0.2187 - val_loss: 0.0816 - val_nmse: 0.1863\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(4.8190705e-05, shape=(), dtype=float32).\n",
      "Epoch 173/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0875 - nmse: 0.2264 - val_loss: 0.0827 - val_nmse: 0.1888\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(4.7711197e-05, shape=(), dtype=float32).\n",
      "Epoch 174/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0883 - nmse: 0.2245 - val_loss: 0.0812 - val_nmse: 0.1851\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(4.723646e-05, shape=(), dtype=float32).\n",
      "Epoch 175/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0874 - nmse: 0.2195 - val_loss: 0.0815 - val_nmse: 0.1857\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(4.6766447e-05, shape=(), dtype=float32).\n",
      "Epoch 176/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0870 - nmse: 0.2293 - val_loss: 0.0817 - val_nmse: 0.1865\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(4.630111e-05, shape=(), dtype=float32).\n",
      "Epoch 177/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0875 - nmse: 0.2205 - val_loss: 0.0808 - val_nmse: 0.1840\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(4.5840403e-05, shape=(), dtype=float32).\n",
      "Epoch 178/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0869 - nmse: 0.2161 - val_loss: 0.0812 - val_nmse: 0.1850\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(4.538428e-05, shape=(), dtype=float32).\n",
      "Epoch 179/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0865 - nmse: 0.2207 - val_loss: 0.0814 - val_nmse: 0.1857\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(4.4932698e-05, shape=(), dtype=float32).\n",
      "Epoch 180/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0865 - nmse: 0.2156 - val_loss: 0.0807 - val_nmse: 0.1836\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(4.448561e-05, shape=(), dtype=float32).\n",
      "Epoch 181/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0861 - nmse: 0.2177 - val_loss: 0.0804 - val_nmse: 0.1827\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(4.404297e-05, shape=(), dtype=float32).\n",
      "Epoch 182/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0857 - nmse: 0.2232 - val_loss: 0.0803 - val_nmse: 0.1823\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(4.360473e-05, shape=(), dtype=float32).\n",
      "Epoch 183/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0856 - nmse: 0.2204 - val_loss: 0.0803 - val_nmse: 0.1823\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(4.3170854e-05, shape=(), dtype=float32).\n",
      "Epoch 184/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0853 - nmse: 0.2191 - val_loss: 0.0801 - val_nmse: 0.1820\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(4.2741296e-05, shape=(), dtype=float32).\n",
      "Epoch 185/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0853 - nmse: 0.2196 - val_loss: 0.0803 - val_nmse: 0.1822\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(4.231601e-05, shape=(), dtype=float32).\n",
      "Epoch 186/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0850 - nmse: 0.2206 - val_loss: 0.0803 - val_nmse: 0.1822\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(4.1894957e-05, shape=(), dtype=float32).\n",
      "Epoch 187/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0849 - nmse: 0.2238 - val_loss: 0.0801 - val_nmse: 0.1817\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(4.147809e-05, shape=(), dtype=float32).\n",
      "Epoch 188/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0848 - nmse: 0.2188 - val_loss: 0.0798 - val_nmse: 0.1812\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(4.1065374e-05, shape=(), dtype=float32).\n",
      "Epoch 189/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0847 - nmse: 0.2099 - val_loss: 0.0797 - val_nmse: 0.1808\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(4.0656763e-05, shape=(), dtype=float32).\n",
      "Epoch 190/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0847 - nmse: 0.2177 - val_loss: 0.0795 - val_nmse: 0.1802\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(4.025222e-05, shape=(), dtype=float32).\n",
      "Epoch 191/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0843 - nmse: 0.2143 - val_loss: 0.0795 - val_nmse: 0.1800\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(3.98517e-05, shape=(), dtype=float32).\n",
      "Epoch 192/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0840 - nmse: 0.2127 - val_loss: 0.0794 - val_nmse: 0.1800\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(3.9455168e-05, shape=(), dtype=float32).\n",
      "Epoch 193/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0839 - nmse: 0.2212 - val_loss: 0.0796 - val_nmse: 0.1803\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(3.906258e-05, shape=(), dtype=float32).\n",
      "Epoch 194/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0839 - nmse: 0.2110 - val_loss: 0.0792 - val_nmse: 0.1794\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to tf.Tensor(3.8673898e-05, shape=(), dtype=float32).\n",
      "Epoch 195/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0835 - nmse: 0.2098 - val_loss: 0.0791 - val_nmse: 0.1792\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to tf.Tensor(3.8289083e-05, shape=(), dtype=float32).\n",
      "Epoch 196/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0834 - nmse: 0.2088 - val_loss: 0.0790 - val_nmse: 0.1788\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to tf.Tensor(3.79081e-05, shape=(), dtype=float32).\n",
      "Epoch 197/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0832 - nmse: 0.2089 - val_loss: 0.0789 - val_nmse: 0.1785\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to tf.Tensor(3.7530906e-05, shape=(), dtype=float32).\n",
      "Epoch 198/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0831 - nmse: 0.2121 - val_loss: 0.0788 - val_nmse: 0.1782\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to tf.Tensor(3.7157464e-05, shape=(), dtype=float32).\n",
      "Epoch 199/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0829 - nmse: 0.2084 - val_loss: 0.0787 - val_nmse: 0.1778\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to tf.Tensor(3.678774e-05, shape=(), dtype=float32).\n",
      "Epoch 200/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0828 - nmse: 0.2159 - val_loss: 0.0786 - val_nmse: 0.1775\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to tf.Tensor(3.6421694e-05, shape=(), dtype=float32).\n",
      "Epoch 201/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0826 - nmse: 0.2098 - val_loss: 0.0786 - val_nmse: 0.1776\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to tf.Tensor(3.605929e-05, shape=(), dtype=float32).\n",
      "Epoch 202/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0825 - nmse: 0.2142 - val_loss: 0.0785 - val_nmse: 0.1772\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to tf.Tensor(3.570049e-05, shape=(), dtype=float32).\n",
      "Epoch 203/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0823 - nmse: 0.2088 - val_loss: 0.0784 - val_nmse: 0.1771\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to tf.Tensor(3.5345263e-05, shape=(), dtype=float32).\n",
      "Epoch 204/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0823 - nmse: 0.2098 - val_loss: 0.0784 - val_nmse: 0.1770\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to tf.Tensor(3.499357e-05, shape=(), dtype=float32).\n",
      "Epoch 205/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0822 - nmse: 0.2116 - val_loss: 0.0783 - val_nmse: 0.1765\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to tf.Tensor(3.4645374e-05, shape=(), dtype=float32).\n",
      "Epoch 206/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0820 - nmse: 0.2154 - val_loss: 0.0782 - val_nmse: 0.1765\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to tf.Tensor(3.4300647e-05, shape=(), dtype=float32).\n",
      "Epoch 207/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0819 - nmse: 0.2072 - val_loss: 0.0781 - val_nmse: 0.1763\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to tf.Tensor(3.3959346e-05, shape=(), dtype=float32).\n",
      "Epoch 208/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0817 - nmse: 0.2121 - val_loss: 0.0780 - val_nmse: 0.1759\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to tf.Tensor(3.3621443e-05, shape=(), dtype=float32).\n",
      "Epoch 209/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0815 - nmse: 0.2056 - val_loss: 0.0779 - val_nmse: 0.1756\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to tf.Tensor(3.3286902e-05, shape=(), dtype=float32).\n",
      "Epoch 210/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0814 - nmse: 0.2056 - val_loss: 0.0779 - val_nmse: 0.1754\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to tf.Tensor(3.295569e-05, shape=(), dtype=float32).\n",
      "Epoch 211/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0813 - nmse: 0.2081 - val_loss: 0.0778 - val_nmse: 0.1752\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to tf.Tensor(3.2627773e-05, shape=(), dtype=float32).\n",
      "Epoch 212/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0811 - nmse: 0.2064 - val_loss: 0.0777 - val_nmse: 0.1750\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to tf.Tensor(3.230312e-05, shape=(), dtype=float32).\n",
      "Epoch 213/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0810 - nmse: 0.2047 - val_loss: 0.0776 - val_nmse: 0.1747\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to tf.Tensor(3.1981697e-05, shape=(), dtype=float32).\n",
      "Epoch 214/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0809 - nmse: 0.2048 - val_loss: 0.0776 - val_nmse: 0.1746\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to tf.Tensor(3.1663472e-05, shape=(), dtype=float32).\n",
      "Epoch 215/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0808 - nmse: 0.2036 - val_loss: 0.0775 - val_nmse: 0.1744\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to tf.Tensor(3.1348412e-05, shape=(), dtype=float32).\n",
      "Epoch 216/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0808 - nmse: 0.2140 - val_loss: 0.0775 - val_nmse: 0.1743\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to tf.Tensor(3.103649e-05, shape=(), dtype=float32).\n",
      "Epoch 217/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0805 - nmse: 0.2040 - val_loss: 0.0775 - val_nmse: 0.1743\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to tf.Tensor(3.0727668e-05, shape=(), dtype=float32).\n",
      "Epoch 218/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0805 - nmse: 0.2075 - val_loss: 0.0774 - val_nmse: 0.1739\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to tf.Tensor(3.0421921e-05, shape=(), dtype=float32).\n",
      "Epoch 219/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0803 - nmse: 0.2091 - val_loss: 0.0773 - val_nmse: 0.1737\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to tf.Tensor(3.0119216e-05, shape=(), dtype=float32).\n",
      "Epoch 220/700\n",
      "21/21 [==============================] - 1s 52ms/sample - loss: 0.0802 - nmse: 0.2054 - val_loss: 0.0772 - val_nmse: 0.1735\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to tf.Tensor(2.9819523e-05, shape=(), dtype=float32).\n",
      "Epoch 221/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0801 - nmse: 0.2052 - val_loss: 0.0772 - val_nmse: 0.1735\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to tf.Tensor(2.9522811e-05, shape=(), dtype=float32).\n",
      "Epoch 222/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0800 - nmse: 0.2016 - val_loss: 0.0771 - val_nmse: 0.1731\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to tf.Tensor(2.9229053e-05, shape=(), dtype=float32).\n",
      "Epoch 223/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0799 - nmse: 0.1973 - val_loss: 0.0770 - val_nmse: 0.1729\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to tf.Tensor(2.8938217e-05, shape=(), dtype=float32).\n",
      "Epoch 224/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0798 - nmse: 0.2061 - val_loss: 0.0769 - val_nmse: 0.1727\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to tf.Tensor(2.8650275e-05, shape=(), dtype=float32).\n",
      "Epoch 225/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0797 - nmse: 0.2018 - val_loss: 0.0769 - val_nmse: 0.1726\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to tf.Tensor(2.8365199e-05, shape=(), dtype=float32).\n",
      "Epoch 226/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0796 - nmse: 0.2104 - val_loss: 0.0769 - val_nmse: 0.1727\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to tf.Tensor(2.8082959e-05, shape=(), dtype=float32).\n",
      "Epoch 227/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0795 - nmse: 0.2013 - val_loss: 0.0769 - val_nmse: 0.1724\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to tf.Tensor(2.7803528e-05, shape=(), dtype=float32).\n",
      "Epoch 228/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0794 - nmse: 0.2001 - val_loss: 0.0767 - val_nmse: 0.1721\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to tf.Tensor(2.7526876e-05, shape=(), dtype=float32).\n",
      "Epoch 229/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0792 - nmse: 0.2106 - val_loss: 0.0767 - val_nmse: 0.1720\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to tf.Tensor(2.7252978e-05, shape=(), dtype=float32).\n",
      "Epoch 230/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0791 - nmse: 0.2006 - val_loss: 0.0767 - val_nmse: 0.1719\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to tf.Tensor(2.6981805e-05, shape=(), dtype=float32).\n",
      "Epoch 231/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0790 - nmse: 0.2019 - val_loss: 0.0765 - val_nmse: 0.1716\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to tf.Tensor(2.671333e-05, shape=(), dtype=float32).\n",
      "Epoch 232/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0788 - nmse: 0.2028 - val_loss: 0.0765 - val_nmse: 0.1715\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to tf.Tensor(2.6447526e-05, shape=(), dtype=float32).\n",
      "Epoch 233/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0788 - nmse: 0.1987 - val_loss: 0.0765 - val_nmse: 0.1714\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to tf.Tensor(2.6184367e-05, shape=(), dtype=float32).\n",
      "Epoch 234/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0787 - nmse: 0.2045 - val_loss: 0.0764 - val_nmse: 0.1711\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to tf.Tensor(2.5923826e-05, shape=(), dtype=float32).\n",
      "Epoch 235/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0786 - nmse: 0.2018 - val_loss: 0.0763 - val_nmse: 0.1708\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to tf.Tensor(2.5665879e-05, shape=(), dtype=float32).\n",
      "Epoch 236/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0785 - nmse: 0.1992 - val_loss: 0.0762 - val_nmse: 0.1707\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to tf.Tensor(2.5410498e-05, shape=(), dtype=float32).\n",
      "Epoch 237/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0784 - nmse: 0.2016 - val_loss: 0.0762 - val_nmse: 0.1705\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to tf.Tensor(2.5157658e-05, shape=(), dtype=float32).\n",
      "Epoch 238/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0782 - nmse: 0.2010 - val_loss: 0.0762 - val_nmse: 0.1705\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to tf.Tensor(2.4907335e-05, shape=(), dtype=float32).\n",
      "Epoch 239/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0781 - nmse: 0.1992 - val_loss: 0.0762 - val_nmse: 0.1704\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to tf.Tensor(2.4659501e-05, shape=(), dtype=float32).\n",
      "Epoch 240/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0780 - nmse: 0.1951 - val_loss: 0.0760 - val_nmse: 0.1701\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to tf.Tensor(2.4414134e-05, shape=(), dtype=float32).\n",
      "Epoch 241/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0779 - nmse: 0.1993 - val_loss: 0.0759 - val_nmse: 0.1699\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to tf.Tensor(2.4171208e-05, shape=(), dtype=float32).\n",
      "Epoch 242/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0778 - nmse: 0.2027 - val_loss: 0.0759 - val_nmse: 0.1698\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to tf.Tensor(2.39307e-05, shape=(), dtype=float32).\n",
      "Epoch 243/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0778 - nmse: 0.2013 - val_loss: 0.0760 - val_nmse: 0.1698\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to tf.Tensor(2.3692583e-05, shape=(), dtype=float32).\n",
      "Epoch 244/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0777 - nmse: 0.1945 - val_loss: 0.0759 - val_nmse: 0.1698\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to tf.Tensor(2.3456836e-05, shape=(), dtype=float32).\n",
      "Epoch 245/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0776 - nmse: 0.2016 - val_loss: 0.0759 - val_nmse: 0.1697\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to tf.Tensor(2.3223436e-05, shape=(), dtype=float32).\n",
      "Epoch 246/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0775 - nmse: 0.1984 - val_loss: 0.0758 - val_nmse: 0.1694\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to tf.Tensor(2.2992357e-05, shape=(), dtype=float32).\n",
      "Epoch 247/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0773 - nmse: 0.2011 - val_loss: 0.0758 - val_nmse: 0.1694\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to tf.Tensor(2.2763577e-05, shape=(), dtype=float32).\n",
      "Epoch 248/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0773 - nmse: 0.1943 - val_loss: 0.0758 - val_nmse: 0.1692\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to tf.Tensor(2.2537075e-05, shape=(), dtype=float32).\n",
      "Epoch 249/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0772 - nmse: 0.1993 - val_loss: 0.0757 - val_nmse: 0.1690\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to tf.Tensor(2.2312826e-05, shape=(), dtype=float32).\n",
      "Epoch 250/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0771 - nmse: 0.1915 - val_loss: 0.0756 - val_nmse: 0.1688\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to tf.Tensor(2.209081e-05, shape=(), dtype=float32).\n",
      "Epoch 251/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0770 - nmse: 0.1916 - val_loss: 0.0756 - val_nmse: 0.1687\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to tf.Tensor(2.1871001e-05, shape=(), dtype=float32).\n",
      "Epoch 252/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0770 - nmse: 0.1944 - val_loss: 0.0756 - val_nmse: 0.1686\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to tf.Tensor(2.1653379e-05, shape=(), dtype=float32).\n",
      "Epoch 253/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0769 - nmse: 0.2042 - val_loss: 0.0754 - val_nmse: 0.1683\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to tf.Tensor(2.1437923e-05, shape=(), dtype=float32).\n",
      "Epoch 254/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0768 - nmse: 0.1973 - val_loss: 0.0754 - val_nmse: 0.1682\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to tf.Tensor(2.122461e-05, shape=(), dtype=float32).\n",
      "Epoch 255/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0766 - nmse: 0.1970 - val_loss: 0.0755 - val_nmse: 0.1682\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to tf.Tensor(2.101342e-05, shape=(), dtype=float32).\n",
      "Epoch 256/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0766 - nmse: 0.1979 - val_loss: 0.0754 - val_nmse: 0.1680\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to tf.Tensor(2.0804333e-05, shape=(), dtype=float32).\n",
      "Epoch 257/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0765 - nmse: 0.1903 - val_loss: 0.0753 - val_nmse: 0.1681\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to tf.Tensor(2.0597325e-05, shape=(), dtype=float32).\n",
      "Epoch 258/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0766 - nmse: 0.1960 - val_loss: 0.0753 - val_nmse: 0.1680\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to tf.Tensor(2.0392377e-05, shape=(), dtype=float32).\n",
      "Epoch 259/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0764 - nmse: 0.1936 - val_loss: 0.0753 - val_nmse: 0.1679\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to tf.Tensor(2.0189469e-05, shape=(), dtype=float32).\n",
      "Epoch 260/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0763 - nmse: 0.1904 - val_loss: 0.0752 - val_nmse: 0.1676\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to tf.Tensor(1.998858e-05, shape=(), dtype=float32).\n",
      "Epoch 261/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0763 - nmse: 0.1951 - val_loss: 0.0751 - val_nmse: 0.1675\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to tf.Tensor(1.978969e-05, shape=(), dtype=float32).\n",
      "Epoch 262/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0762 - nmse: 0.1880 - val_loss: 0.0751 - val_nmse: 0.1673\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to tf.Tensor(1.9592779e-05, shape=(), dtype=float32).\n",
      "Epoch 263/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0760 - nmse: 0.1903 - val_loss: 0.0750 - val_nmse: 0.1671\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to tf.Tensor(1.9397827e-05, shape=(), dtype=float32).\n",
      "Epoch 264/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0760 - nmse: 0.1942 - val_loss: 0.0749 - val_nmse: 0.1670\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to tf.Tensor(1.9204814e-05, shape=(), dtype=float32).\n",
      "Epoch 265/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0759 - nmse: 0.1984 - val_loss: 0.0750 - val_nmse: 0.1671\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to tf.Tensor(1.9013722e-05, shape=(), dtype=float32).\n",
      "Epoch 266/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0759 - nmse: 0.1959 - val_loss: 0.0749 - val_nmse: 0.1669\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to tf.Tensor(1.882453e-05, shape=(), dtype=float32).\n",
      "Epoch 267/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0758 - nmse: 0.1930 - val_loss: 0.0750 - val_nmse: 0.1669\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to tf.Tensor(1.8637222e-05, shape=(), dtype=float32).\n",
      "Epoch 268/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0757 - nmse: 0.1921 - val_loss: 0.0749 - val_nmse: 0.1667\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to tf.Tensor(1.8451778e-05, shape=(), dtype=float32).\n",
      "Epoch 269/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0756 - nmse: 0.1926 - val_loss: 0.0749 - val_nmse: 0.1669\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to tf.Tensor(1.8268178e-05, shape=(), dtype=float32).\n",
      "Epoch 270/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0756 - nmse: 0.1946 - val_loss: 0.0749 - val_nmse: 0.1666\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to tf.Tensor(1.8086406e-05, shape=(), dtype=float32).\n",
      "Epoch 271/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0755 - nmse: 0.1915 - val_loss: 0.0748 - val_nmse: 0.1666\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to tf.Tensor(1.7906443e-05, shape=(), dtype=float32).\n",
      "Epoch 272/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0755 - nmse: 0.1925 - val_loss: 0.0748 - val_nmse: 0.1665\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to tf.Tensor(1.7728269e-05, shape=(), dtype=float32).\n",
      "Epoch 273/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0754 - nmse: 0.1884 - val_loss: 0.0747 - val_nmse: 0.1664\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to tf.Tensor(1.7551869e-05, shape=(), dtype=float32).\n",
      "Epoch 274/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0753 - nmse: 0.1932 - val_loss: 0.0746 - val_nmse: 0.1661\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to tf.Tensor(1.7377224e-05, shape=(), dtype=float32).\n",
      "Epoch 275/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0752 - nmse: 0.1958 - val_loss: 0.0746 - val_nmse: 0.1659\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to tf.Tensor(1.7204316e-05, shape=(), dtype=float32).\n",
      "Epoch 276/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0752 - nmse: 0.1920 - val_loss: 0.0747 - val_nmse: 0.1660\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to tf.Tensor(1.703313e-05, shape=(), dtype=float32).\n",
      "Epoch 277/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0751 - nmse: 0.1871 - val_loss: 0.0746 - val_nmse: 0.1658\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to tf.Tensor(1.6863647e-05, shape=(), dtype=float32).\n",
      "Epoch 278/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0750 - nmse: 0.1906 - val_loss: 0.0745 - val_nmse: 0.1656\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to tf.Tensor(1.669585e-05, shape=(), dtype=float32).\n",
      "Epoch 279/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0749 - nmse: 0.1905 - val_loss: 0.0745 - val_nmse: 0.1655\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to tf.Tensor(1.6529722e-05, shape=(), dtype=float32).\n",
      "Epoch 280/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0749 - nmse: 0.1941 - val_loss: 0.0745 - val_nmse: 0.1654\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to tf.Tensor(1.6365248e-05, shape=(), dtype=float32).\n",
      "Epoch 281/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0748 - nmse: 0.1940 - val_loss: 0.0744 - val_nmse: 0.1653\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to tf.Tensor(1.620241e-05, shape=(), dtype=float32).\n",
      "Epoch 282/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0748 - nmse: 0.1928 - val_loss: 0.0744 - val_nmse: 0.1653\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to tf.Tensor(1.6041193e-05, shape=(), dtype=float32).\n",
      "Epoch 283/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0747 - nmse: 0.1906 - val_loss: 0.0744 - val_nmse: 0.1652\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to tf.Tensor(1.588158e-05, shape=(), dtype=float32).\n",
      "Epoch 284/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0746 - nmse: 0.1959 - val_loss: 0.0744 - val_nmse: 0.1652\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to tf.Tensor(1.5723555e-05, shape=(), dtype=float32).\n",
      "Epoch 285/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0745 - nmse: 0.1908 - val_loss: 0.0744 - val_nmse: 0.1652\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to tf.Tensor(1.5567102e-05, shape=(), dtype=float32).\n",
      "Epoch 286/700\n",
      "21/21 [==============================] - 1s 51ms/sample - loss: 0.0745 - nmse: 0.1946 - val_loss: 0.0743 - val_nmse: 0.1650\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to tf.Tensor(1.5412206e-05, shape=(), dtype=float32).\n",
      "Epoch 287/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0744 - nmse: 0.1920 - val_loss: 0.0743 - val_nmse: 0.1650\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to tf.Tensor(1.525885e-05, shape=(), dtype=float32).\n",
      "Epoch 288/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0743 - nmse: 0.1893 - val_loss: 0.0743 - val_nmse: 0.1650\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to tf.Tensor(1.5107022e-05, shape=(), dtype=float32).\n",
      "Epoch 289/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0743 - nmse: 0.1930 - val_loss: 0.0743 - val_nmse: 0.1649\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to tf.Tensor(1.4956703e-05, shape=(), dtype=float32).\n",
      "Epoch 290/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0743 - nmse: 0.1864 - val_loss: 0.0742 - val_nmse: 0.1649\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to tf.Tensor(1.4807881e-05, shape=(), dtype=float32).\n",
      "Epoch 291/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0742 - nmse: 0.1895 - val_loss: 0.0741 - val_nmse: 0.1647\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to tf.Tensor(1.4660539e-05, shape=(), dtype=float32).\n",
      "Epoch 292/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0741 - nmse: 0.1867 - val_loss: 0.0741 - val_nmse: 0.1646\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to tf.Tensor(1.4514663e-05, shape=(), dtype=float32).\n",
      "Epoch 293/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0741 - nmse: 0.1922 - val_loss: 0.0741 - val_nmse: 0.1644\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to tf.Tensor(1.4370239e-05, shape=(), dtype=float32).\n",
      "Epoch 294/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0740 - nmse: 0.1885 - val_loss: 0.0741 - val_nmse: 0.1645\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to tf.Tensor(1.4227252e-05, shape=(), dtype=float32).\n",
      "Epoch 295/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0740 - nmse: 0.1882 - val_loss: 0.0741 - val_nmse: 0.1644\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to tf.Tensor(1.4085687e-05, shape=(), dtype=float32).\n",
      "Epoch 296/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0739 - nmse: 0.1873 - val_loss: 0.0740 - val_nmse: 0.1642\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to tf.Tensor(1.39455315e-05, shape=(), dtype=float32).\n",
      "Epoch 297/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0739 - nmse: 0.1894 - val_loss: 0.0740 - val_nmse: 0.1641\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to tf.Tensor(1.3806771e-05, shape=(), dtype=float32).\n",
      "Epoch 298/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0738 - nmse: 0.1959 - val_loss: 0.0740 - val_nmse: 0.1641\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to tf.Tensor(1.3669391e-05, shape=(), dtype=float32).\n",
      "Epoch 299/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0738 - nmse: 0.1875 - val_loss: 0.0739 - val_nmse: 0.1639\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to tf.Tensor(1.3533378e-05, shape=(), dtype=float32).\n",
      "Epoch 300/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0737 - nmse: 0.1820 - val_loss: 0.0739 - val_nmse: 0.1640\n",
      "\n",
      "Epoch 00301: LearningRateScheduler reducing learning rate to tf.Tensor(1.3398718e-05, shape=(), dtype=float32).\n",
      "Epoch 301/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0736 - nmse: 0.1907 - val_loss: 0.0739 - val_nmse: 0.1638\n",
      "\n",
      "Epoch 00302: LearningRateScheduler reducing learning rate to tf.Tensor(1.3265398e-05, shape=(), dtype=float32).\n",
      "Epoch 302/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0736 - nmse: 0.1903 - val_loss: 0.0739 - val_nmse: 0.1639\n",
      "\n",
      "Epoch 00303: LearningRateScheduler reducing learning rate to tf.Tensor(1.3133404e-05, shape=(), dtype=float32).\n",
      "Epoch 303/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0735 - nmse: 0.1899 - val_loss: 0.0738 - val_nmse: 0.1637\n",
      "\n",
      "Epoch 00304: LearningRateScheduler reducing learning rate to tf.Tensor(1.3002724e-05, shape=(), dtype=float32).\n",
      "Epoch 304/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0735 - nmse: 0.1843 - val_loss: 0.0738 - val_nmse: 0.1637\n",
      "\n",
      "Epoch 00305: LearningRateScheduler reducing learning rate to tf.Tensor(1.2873344e-05, shape=(), dtype=float32).\n",
      "Epoch 305/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0734 - nmse: 0.1925 - val_loss: 0.0738 - val_nmse: 0.1636\n",
      "\n",
      "Epoch 00306: LearningRateScheduler reducing learning rate to tf.Tensor(1.2745251e-05, shape=(), dtype=float32).\n",
      "Epoch 306/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0734 - nmse: 0.1907 - val_loss: 0.0738 - val_nmse: 0.1635\n",
      "\n",
      "Epoch 00307: LearningRateScheduler reducing learning rate to tf.Tensor(1.2618433e-05, shape=(), dtype=float32).\n",
      "Epoch 307/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0733 - nmse: 0.1872 - val_loss: 0.0737 - val_nmse: 0.1634\n",
      "\n",
      "Epoch 00308: LearningRateScheduler reducing learning rate to tf.Tensor(1.2492877e-05, shape=(), dtype=float32).\n",
      "Epoch 308/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0732 - nmse: 0.1857 - val_loss: 0.0738 - val_nmse: 0.1634\n",
      "\n",
      "Epoch 00309: LearningRateScheduler reducing learning rate to tf.Tensor(1.236857e-05, shape=(), dtype=float32).\n",
      "Epoch 309/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0732 - nmse: 0.1844 - val_loss: 0.0737 - val_nmse: 0.1633\n",
      "\n",
      "Epoch 00310: LearningRateScheduler reducing learning rate to tf.Tensor(1.22455e-05, shape=(), dtype=float32).\n",
      "Epoch 310/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0732 - nmse: 0.1849 - val_loss: 0.0737 - val_nmse: 0.1632\n",
      "\n",
      "Epoch 00311: LearningRateScheduler reducing learning rate to tf.Tensor(1.2123655e-05, shape=(), dtype=float32).\n",
      "Epoch 311/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0731 - nmse: 0.1857 - val_loss: 0.0737 - val_nmse: 0.1631\n",
      "\n",
      "Epoch 00312: LearningRateScheduler reducing learning rate to tf.Tensor(1.2003023e-05, shape=(), dtype=float32).\n",
      "Epoch 312/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0730 - nmse: 0.1848 - val_loss: 0.0736 - val_nmse: 0.1631\n",
      "\n",
      "Epoch 00313: LearningRateScheduler reducing learning rate to tf.Tensor(1.188359e-05, shape=(), dtype=float32).\n",
      "Epoch 313/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0730 - nmse: 0.1857 - val_loss: 0.0736 - val_nmse: 0.1630\n",
      "\n",
      "Epoch 00314: LearningRateScheduler reducing learning rate to tf.Tensor(1.1765345e-05, shape=(), dtype=float32).\n",
      "Epoch 314/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0730 - nmse: 0.1841 - val_loss: 0.0736 - val_nmse: 0.1629\n",
      "\n",
      "Epoch 00315: LearningRateScheduler reducing learning rate to tf.Tensor(1.1648278e-05, shape=(), dtype=float32).\n",
      "Epoch 315/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0729 - nmse: 0.1821 - val_loss: 0.0736 - val_nmse: 0.1629\n",
      "\n",
      "Epoch 00316: LearningRateScheduler reducing learning rate to tf.Tensor(1.1532375e-05, shape=(), dtype=float32).\n",
      "Epoch 316/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0729 - nmse: 0.1818 - val_loss: 0.0735 - val_nmse: 0.1627\n",
      "\n",
      "Epoch 00317: LearningRateScheduler reducing learning rate to tf.Tensor(1.1417625e-05, shape=(), dtype=float32).\n",
      "Epoch 317/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0728 - nmse: 0.1806 - val_loss: 0.0735 - val_nmse: 0.1626\n",
      "\n",
      "Epoch 00318: LearningRateScheduler reducing learning rate to tf.Tensor(1.1304017e-05, shape=(), dtype=float32).\n",
      "Epoch 318/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0728 - nmse: 0.1843 - val_loss: 0.0735 - val_nmse: 0.1627\n",
      "\n",
      "Epoch 00319: LearningRateScheduler reducing learning rate to tf.Tensor(1.119154e-05, shape=(), dtype=float32).\n",
      "Epoch 319/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0728 - nmse: 0.1872 - val_loss: 0.0735 - val_nmse: 0.1625\n",
      "\n",
      "Epoch 00320: LearningRateScheduler reducing learning rate to tf.Tensor(1.1080181e-05, shape=(), dtype=float32).\n",
      "Epoch 320/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0727 - nmse: 0.1872 - val_loss: 0.0734 - val_nmse: 0.1624\n",
      "\n",
      "Epoch 00321: LearningRateScheduler reducing learning rate to tf.Tensor(1.0969931e-05, shape=(), dtype=float32).\n",
      "Epoch 321/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0726 - nmse: 0.1928 - val_loss: 0.0735 - val_nmse: 0.1625\n",
      "\n",
      "Epoch 00322: LearningRateScheduler reducing learning rate to tf.Tensor(1.0860778e-05, shape=(), dtype=float32).\n",
      "Epoch 322/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0726 - nmse: 0.1845 - val_loss: 0.0735 - val_nmse: 0.1625\n",
      "\n",
      "Epoch 00323: LearningRateScheduler reducing learning rate to tf.Tensor(1.0752711e-05, shape=(), dtype=float32).\n",
      "Epoch 323/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0726 - nmse: 0.1812 - val_loss: 0.0734 - val_nmse: 0.1624\n",
      "\n",
      "Epoch 00324: LearningRateScheduler reducing learning rate to tf.Tensor(1.0645719e-05, shape=(), dtype=float32).\n",
      "Epoch 324/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0726 - nmse: 0.1868 - val_loss: 0.0734 - val_nmse: 0.1624\n",
      "\n",
      "Epoch 00325: LearningRateScheduler reducing learning rate to tf.Tensor(1.0539792e-05, shape=(), dtype=float32).\n",
      "Epoch 325/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0725 - nmse: 0.1856 - val_loss: 0.0734 - val_nmse: 0.1623\n",
      "\n",
      "Epoch 00326: LearningRateScheduler reducing learning rate to tf.Tensor(1.0434919e-05, shape=(), dtype=float32).\n",
      "Epoch 326/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0725 - nmse: 0.1893 - val_loss: 0.0734 - val_nmse: 0.1622\n",
      "\n",
      "Epoch 00327: LearningRateScheduler reducing learning rate to tf.Tensor(1.03310895e-05, shape=(), dtype=float32).\n",
      "Epoch 327/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0724 - nmse: 0.1872 - val_loss: 0.0734 - val_nmse: 0.1621\n",
      "\n",
      "Epoch 00328: LearningRateScheduler reducing learning rate to tf.Tensor(1.0228293e-05, shape=(), dtype=float32).\n",
      "Epoch 328/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0724 - nmse: 0.1858 - val_loss: 0.0733 - val_nmse: 0.1620\n",
      "\n",
      "Epoch 00329: LearningRateScheduler reducing learning rate to tf.Tensor(1.012652e-05, shape=(), dtype=float32).\n",
      "Epoch 329/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0723 - nmse: 0.1882 - val_loss: 0.0733 - val_nmse: 0.1621\n",
      "\n",
      "Epoch 00330: LearningRateScheduler reducing learning rate to tf.Tensor(1.00257585e-05, shape=(), dtype=float32).\n",
      "Epoch 330/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0723 - nmse: 0.1827 - val_loss: 0.0733 - val_nmse: 0.1620\n",
      "\n",
      "Epoch 00331: LearningRateScheduler reducing learning rate to tf.Tensor(9.926e-06, shape=(), dtype=float32).\n",
      "Epoch 331/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0723 - nmse: 0.1846 - val_loss: 0.0733 - val_nmse: 0.1619\n",
      "\n",
      "Epoch 00332: LearningRateScheduler reducing learning rate to tf.Tensor(9.827234e-06, shape=(), dtype=float32).\n",
      "Epoch 332/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0722 - nmse: 0.1864 - val_loss: 0.0732 - val_nmse: 0.1619\n",
      "\n",
      "Epoch 00333: LearningRateScheduler reducing learning rate to tf.Tensor(9.7294505e-06, shape=(), dtype=float32).\n",
      "Epoch 333/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0721 - nmse: 0.1837 - val_loss: 0.0733 - val_nmse: 0.1619\n",
      "\n",
      "Epoch 00334: LearningRateScheduler reducing learning rate to tf.Tensor(9.63264e-06, shape=(), dtype=float32).\n",
      "Epoch 334/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0721 - nmse: 0.1857 - val_loss: 0.0732 - val_nmse: 0.1618\n",
      "\n",
      "Epoch 00335: LearningRateScheduler reducing learning rate to tf.Tensor(9.536793e-06, shape=(), dtype=float32).\n",
      "Epoch 335/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0721 - nmse: 0.1861 - val_loss: 0.0732 - val_nmse: 0.1618\n",
      "\n",
      "Epoch 00336: LearningRateScheduler reducing learning rate to tf.Tensor(9.4419e-06, shape=(), dtype=float32).\n",
      "Epoch 336/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0721 - nmse: 0.1881 - val_loss: 0.0732 - val_nmse: 0.1618\n",
      "\n",
      "Epoch 00337: LearningRateScheduler reducing learning rate to tf.Tensor(9.347951e-06, shape=(), dtype=float32).\n",
      "Epoch 337/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0720 - nmse: 0.1854 - val_loss: 0.0732 - val_nmse: 0.1618\n",
      "\n",
      "Epoch 00338: LearningRateScheduler reducing learning rate to tf.Tensor(9.254937e-06, shape=(), dtype=float32).\n",
      "Epoch 338/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0720 - nmse: 0.1865 - val_loss: 0.0731 - val_nmse: 0.1616\n",
      "\n",
      "Epoch 00339: LearningRateScheduler reducing learning rate to tf.Tensor(9.162848e-06, shape=(), dtype=float32).\n",
      "Epoch 339/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0719 - nmse: 0.1841 - val_loss: 0.0731 - val_nmse: 0.1616\n",
      "\n",
      "Epoch 00340: LearningRateScheduler reducing learning rate to tf.Tensor(9.071676e-06, shape=(), dtype=float32).\n",
      "Epoch 340/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0719 - nmse: 0.1875 - val_loss: 0.0731 - val_nmse: 0.1616\n",
      "\n",
      "Epoch 00341: LearningRateScheduler reducing learning rate to tf.Tensor(8.98141e-06, shape=(), dtype=float32).\n",
      "Epoch 341/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0718 - nmse: 0.1864 - val_loss: 0.0731 - val_nmse: 0.1615\n",
      "\n",
      "Epoch 00342: LearningRateScheduler reducing learning rate to tf.Tensor(8.892043e-06, shape=(), dtype=float32).\n",
      "Epoch 342/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0718 - nmse: 0.1863 - val_loss: 0.0731 - val_nmse: 0.1614\n",
      "\n",
      "Epoch 00343: LearningRateScheduler reducing learning rate to tf.Tensor(8.803566e-06, shape=(), dtype=float32).\n",
      "Epoch 343/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0718 - nmse: 0.1827 - val_loss: 0.0731 - val_nmse: 0.1614\n",
      "\n",
      "Epoch 00344: LearningRateScheduler reducing learning rate to tf.Tensor(8.715969e-06, shape=(), dtype=float32).\n",
      "Epoch 344/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0717 - nmse: 0.1824 - val_loss: 0.0731 - val_nmse: 0.1613\n",
      "\n",
      "Epoch 00345: LearningRateScheduler reducing learning rate to tf.Tensor(8.629243e-06, shape=(), dtype=float32).\n",
      "Epoch 345/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0717 - nmse: 0.1871 - val_loss: 0.0730 - val_nmse: 0.1613\n",
      "\n",
      "Epoch 00346: LearningRateScheduler reducing learning rate to tf.Tensor(8.54338e-06, shape=(), dtype=float32).\n",
      "Epoch 346/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0717 - nmse: 0.1824 - val_loss: 0.0730 - val_nmse: 0.1613\n",
      "\n",
      "Epoch 00347: LearningRateScheduler reducing learning rate to tf.Tensor(8.458372e-06, shape=(), dtype=float32).\n",
      "Epoch 347/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0716 - nmse: 0.1894 - val_loss: 0.0730 - val_nmse: 0.1612\n",
      "\n",
      "Epoch 00348: LearningRateScheduler reducing learning rate to tf.Tensor(8.374209e-06, shape=(), dtype=float32).\n",
      "Epoch 348/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0716 - nmse: 0.1837 - val_loss: 0.0730 - val_nmse: 0.1612\n",
      "\n",
      "Epoch 00349: LearningRateScheduler reducing learning rate to tf.Tensor(8.290884e-06, shape=(), dtype=float32).\n",
      "Epoch 349/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0716 - nmse: 0.1819 - val_loss: 0.0730 - val_nmse: 0.1612\n",
      "\n",
      "Epoch 00350: LearningRateScheduler reducing learning rate to tf.Tensor(8.208388e-06, shape=(), dtype=float32).\n",
      "Epoch 350/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0715 - nmse: 0.1791 - val_loss: 0.0730 - val_nmse: 0.1612\n",
      "\n",
      "Epoch 00351: LearningRateScheduler reducing learning rate to tf.Tensor(8.126713e-06, shape=(), dtype=float32).\n",
      "Epoch 351/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0715 - nmse: 0.1801 - val_loss: 0.0729 - val_nmse: 0.1610\n",
      "\n",
      "Epoch 00352: LearningRateScheduler reducing learning rate to tf.Tensor(8.04585e-06, shape=(), dtype=float32).\n",
      "Epoch 352/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0715 - nmse: 0.1818 - val_loss: 0.0729 - val_nmse: 0.1610\n",
      "\n",
      "Epoch 00353: LearningRateScheduler reducing learning rate to tf.Tensor(7.965792e-06, shape=(), dtype=float32).\n",
      "Epoch 353/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0714 - nmse: 0.1859 - val_loss: 0.0730 - val_nmse: 0.1610\n",
      "\n",
      "Epoch 00354: LearningRateScheduler reducing learning rate to tf.Tensor(7.8865305e-06, shape=(), dtype=float32).\n",
      "Epoch 354/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0714 - nmse: 0.1804 - val_loss: 0.0729 - val_nmse: 0.1609\n",
      "\n",
      "Epoch 00355: LearningRateScheduler reducing learning rate to tf.Tensor(7.8080575e-06, shape=(), dtype=float32).\n",
      "Epoch 355/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0713 - nmse: 0.1777 - val_loss: 0.0729 - val_nmse: 0.1608\n",
      "\n",
      "Epoch 00356: LearningRateScheduler reducing learning rate to tf.Tensor(7.730366e-06, shape=(), dtype=float32).\n",
      "Epoch 356/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0714 - nmse: 0.1877 - val_loss: 0.0728 - val_nmse: 0.1608\n",
      "\n",
      "Epoch 00357: LearningRateScheduler reducing learning rate to tf.Tensor(7.653447e-06, shape=(), dtype=float32).\n",
      "Epoch 357/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0713 - nmse: 0.1809 - val_loss: 0.0728 - val_nmse: 0.1607\n",
      "\n",
      "Epoch 00358: LearningRateScheduler reducing learning rate to tf.Tensor(7.5772937e-06, shape=(), dtype=float32).\n",
      "Epoch 358/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0713 - nmse: 0.1818 - val_loss: 0.0729 - val_nmse: 0.1608\n",
      "\n",
      "Epoch 00359: LearningRateScheduler reducing learning rate to tf.Tensor(7.501898e-06, shape=(), dtype=float32).\n",
      "Epoch 359/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0713 - nmse: 0.1821 - val_loss: 0.0729 - val_nmse: 0.1607\n",
      "\n",
      "Epoch 00360: LearningRateScheduler reducing learning rate to tf.Tensor(7.4272525e-06, shape=(), dtype=float32).\n",
      "Epoch 360/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0712 - nmse: 0.1783 - val_loss: 0.0728 - val_nmse: 0.1606\n",
      "\n",
      "Epoch 00361: LearningRateScheduler reducing learning rate to tf.Tensor(7.3533497e-06, shape=(), dtype=float32).\n",
      "Epoch 361/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0712 - nmse: 0.1826 - val_loss: 0.0728 - val_nmse: 0.1606\n",
      "\n",
      "Epoch 00362: LearningRateScheduler reducing learning rate to tf.Tensor(7.280182e-06, shape=(), dtype=float32).\n",
      "Epoch 362/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0711 - nmse: 0.1841 - val_loss: 0.0728 - val_nmse: 0.1605\n",
      "\n",
      "Epoch 00363: LearningRateScheduler reducing learning rate to tf.Tensor(7.207743e-06, shape=(), dtype=float32).\n",
      "Epoch 363/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0711 - nmse: 0.1794 - val_loss: 0.0728 - val_nmse: 0.1606\n",
      "\n",
      "Epoch 00364: LearningRateScheduler reducing learning rate to tf.Tensor(7.136024e-06, shape=(), dtype=float32).\n",
      "Epoch 364/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0711 - nmse: 0.1841 - val_loss: 0.0728 - val_nmse: 0.1605\n",
      "\n",
      "Epoch 00365: LearningRateScheduler reducing learning rate to tf.Tensor(7.065019e-06, shape=(), dtype=float32).\n",
      "Epoch 365/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0711 - nmse: 0.1816 - val_loss: 0.0727 - val_nmse: 0.1605\n",
      "\n",
      "Epoch 00366: LearningRateScheduler reducing learning rate to tf.Tensor(6.9947205e-06, shape=(), dtype=float32).\n",
      "Epoch 366/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0711 - nmse: 0.1865 - val_loss: 0.0727 - val_nmse: 0.1604\n",
      "\n",
      "Epoch 00367: LearningRateScheduler reducing learning rate to tf.Tensor(6.9251214e-06, shape=(), dtype=float32).\n",
      "Epoch 367/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0710 - nmse: 0.1867 - val_loss: 0.0728 - val_nmse: 0.1604\n",
      "\n",
      "Epoch 00368: LearningRateScheduler reducing learning rate to tf.Tensor(6.856215e-06, shape=(), dtype=float32).\n",
      "Epoch 368/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0710 - nmse: 0.1811 - val_loss: 0.0727 - val_nmse: 0.1604\n",
      "\n",
      "Epoch 00369: LearningRateScheduler reducing learning rate to tf.Tensor(6.787994e-06, shape=(), dtype=float32).\n",
      "Epoch 369/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0710 - nmse: 0.1834 - val_loss: 0.0727 - val_nmse: 0.1603\n",
      "\n",
      "Epoch 00370: LearningRateScheduler reducing learning rate to tf.Tensor(6.720452e-06, shape=(), dtype=float32).\n",
      "Epoch 370/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0709 - nmse: 0.1776 - val_loss: 0.0727 - val_nmse: 0.1603\n",
      "\n",
      "Epoch 00371: LearningRateScheduler reducing learning rate to tf.Tensor(6.6535817e-06, shape=(), dtype=float32).\n",
      "Epoch 371/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0709 - nmse: 0.1870 - val_loss: 0.0727 - val_nmse: 0.1603\n",
      "\n",
      "Epoch 00372: LearningRateScheduler reducing learning rate to tf.Tensor(6.5873774e-06, shape=(), dtype=float32).\n",
      "Epoch 372/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0709 - nmse: 0.1829 - val_loss: 0.0727 - val_nmse: 0.1602\n",
      "\n",
      "Epoch 00373: LearningRateScheduler reducing learning rate to tf.Tensor(6.5218314e-06, shape=(), dtype=float32).\n",
      "Epoch 373/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0708 - nmse: 0.1789 - val_loss: 0.0727 - val_nmse: 0.1603\n",
      "\n",
      "Epoch 00374: LearningRateScheduler reducing learning rate to tf.Tensor(6.4569376e-06, shape=(), dtype=float32).\n",
      "Epoch 374/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0708 - nmse: 0.1816 - val_loss: 0.0727 - val_nmse: 0.1602\n",
      "\n",
      "Epoch 00375: LearningRateScheduler reducing learning rate to tf.Tensor(6.3926896e-06, shape=(), dtype=float32).\n",
      "Epoch 375/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0708 - nmse: 0.1808 - val_loss: 0.0726 - val_nmse: 0.1601\n",
      "\n",
      "Epoch 00376: LearningRateScheduler reducing learning rate to tf.Tensor(6.329081e-06, shape=(), dtype=float32).\n",
      "Epoch 376/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0707 - nmse: 0.1782 - val_loss: 0.0726 - val_nmse: 0.1600\n",
      "\n",
      "Epoch 00377: LearningRateScheduler reducing learning rate to tf.Tensor(6.266105e-06, shape=(), dtype=float32).\n",
      "Epoch 377/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0707 - nmse: 0.1814 - val_loss: 0.0726 - val_nmse: 0.1600\n",
      "\n",
      "Epoch 00378: LearningRateScheduler reducing learning rate to tf.Tensor(6.203756e-06, shape=(), dtype=float32).\n",
      "Epoch 378/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0707 - nmse: 0.1783 - val_loss: 0.0727 - val_nmse: 0.1601\n",
      "\n",
      "Epoch 00379: LearningRateScheduler reducing learning rate to tf.Tensor(6.1420274e-06, shape=(), dtype=float32).\n",
      "Epoch 379/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0707 - nmse: 0.1833 - val_loss: 0.0726 - val_nmse: 0.1599\n",
      "\n",
      "Epoch 00380: LearningRateScheduler reducing learning rate to tf.Tensor(6.080913e-06, shape=(), dtype=float32).\n",
      "Epoch 380/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0706 - nmse: 0.1773 - val_loss: 0.0726 - val_nmse: 0.1599\n",
      "\n",
      "Epoch 00381: LearningRateScheduler reducing learning rate to tf.Tensor(6.0204065e-06, shape=(), dtype=float32).\n",
      "Epoch 381/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0706 - nmse: 0.1791 - val_loss: 0.0726 - val_nmse: 0.1598\n",
      "\n",
      "Epoch 00382: LearningRateScheduler reducing learning rate to tf.Tensor(5.960502e-06, shape=(), dtype=float32).\n",
      "Epoch 382/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0706 - nmse: 0.1802 - val_loss: 0.0726 - val_nmse: 0.1598\n",
      "\n",
      "Epoch 00383: LearningRateScheduler reducing learning rate to tf.Tensor(5.901194e-06, shape=(), dtype=float32).\n",
      "Epoch 383/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0706 - nmse: 0.1845 - val_loss: 0.0726 - val_nmse: 0.1598\n",
      "\n",
      "Epoch 00384: LearningRateScheduler reducing learning rate to tf.Tensor(5.8424757e-06, shape=(), dtype=float32).\n",
      "Epoch 384/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0705 - nmse: 0.1782 - val_loss: 0.0726 - val_nmse: 0.1598\n",
      "\n",
      "Epoch 00385: LearningRateScheduler reducing learning rate to tf.Tensor(5.7843417e-06, shape=(), dtype=float32).\n",
      "Epoch 385/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0705 - nmse: 0.1766 - val_loss: 0.0725 - val_nmse: 0.1597\n",
      "\n",
      "Epoch 00386: LearningRateScheduler reducing learning rate to tf.Tensor(5.726786e-06, shape=(), dtype=float32).\n",
      "Epoch 386/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0705 - nmse: 0.1769 - val_loss: 0.0725 - val_nmse: 0.1597\n",
      "\n",
      "Epoch 00387: LearningRateScheduler reducing learning rate to tf.Tensor(5.6698036e-06, shape=(), dtype=float32).\n",
      "Epoch 387/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0705 - nmse: 0.1778 - val_loss: 0.0725 - val_nmse: 0.1597\n",
      "\n",
      "Epoch 00388: LearningRateScheduler reducing learning rate to tf.Tensor(5.6133877e-06, shape=(), dtype=float32).\n",
      "Epoch 388/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0704 - nmse: 0.1788 - val_loss: 0.0725 - val_nmse: 0.1597\n",
      "\n",
      "Epoch 00389: LearningRateScheduler reducing learning rate to tf.Tensor(5.5575333e-06, shape=(), dtype=float32).\n",
      "Epoch 389/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0704 - nmse: 0.1804 - val_loss: 0.0725 - val_nmse: 0.1596\n",
      "\n",
      "Epoch 00390: LearningRateScheduler reducing learning rate to tf.Tensor(5.5022347e-06, shape=(), dtype=float32).\n",
      "Epoch 390/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0704 - nmse: 0.1811 - val_loss: 0.0724 - val_nmse: 0.1595\n",
      "\n",
      "Epoch 00391: LearningRateScheduler reducing learning rate to tf.Tensor(5.4474863e-06, shape=(), dtype=float32).\n",
      "Epoch 391/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0704 - nmse: 0.1784 - val_loss: 0.0724 - val_nmse: 0.1595\n",
      "\n",
      "Epoch 00392: LearningRateScheduler reducing learning rate to tf.Tensor(5.3932827e-06, shape=(), dtype=float32).\n",
      "Epoch 392/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0703 - nmse: 0.1735 - val_loss: 0.0724 - val_nmse: 0.1595\n",
      "\n",
      "Epoch 00393: LearningRateScheduler reducing learning rate to tf.Tensor(5.3396184e-06, shape=(), dtype=float32).\n",
      "Epoch 393/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0703 - nmse: 0.1823 - val_loss: 0.0724 - val_nmse: 0.1594\n",
      "\n",
      "Epoch 00394: LearningRateScheduler reducing learning rate to tf.Tensor(5.286488e-06, shape=(), dtype=float32).\n",
      "Epoch 394/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0703 - nmse: 0.1810 - val_loss: 0.0724 - val_nmse: 0.1594\n",
      "\n",
      "Epoch 00395: LearningRateScheduler reducing learning rate to tf.Tensor(5.2338864e-06, shape=(), dtype=float32).\n",
      "Epoch 395/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0703 - nmse: 0.1760 - val_loss: 0.0724 - val_nmse: 0.1594\n",
      "\n",
      "Epoch 00396: LearningRateScheduler reducing learning rate to tf.Tensor(5.1818083e-06, shape=(), dtype=float32).\n",
      "Epoch 396/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0703 - nmse: 0.1844 - val_loss: 0.0724 - val_nmse: 0.1594\n",
      "\n",
      "Epoch 00397: LearningRateScheduler reducing learning rate to tf.Tensor(5.130248e-06, shape=(), dtype=float32).\n",
      "Epoch 397/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0702 - nmse: 0.1774 - val_loss: 0.0724 - val_nmse: 0.1593\n",
      "\n",
      "Epoch 00398: LearningRateScheduler reducing learning rate to tf.Tensor(5.079201e-06, shape=(), dtype=float32).\n",
      "Epoch 398/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0702 - nmse: 0.1790 - val_loss: 0.0724 - val_nmse: 0.1593\n",
      "\n",
      "Epoch 00399: LearningRateScheduler reducing learning rate to tf.Tensor(5.0286617e-06, shape=(), dtype=float32).\n",
      "Epoch 399/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0702 - nmse: 0.1824 - val_loss: 0.0724 - val_nmse: 0.1593\n",
      "\n",
      "Epoch 00400: LearningRateScheduler reducing learning rate to tf.Tensor(4.9786254e-06, shape=(), dtype=float32).\n",
      "Epoch 400/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0702 - nmse: 0.1802 - val_loss: 0.0724 - val_nmse: 0.1593\n",
      "\n",
      "Epoch 00401: LearningRateScheduler reducing learning rate to tf.Tensor(4.929087e-06, shape=(), dtype=float32).\n",
      "Epoch 401/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0701 - nmse: 0.1809 - val_loss: 0.0724 - val_nmse: 0.1592\n",
      "\n",
      "Epoch 00402: LearningRateScheduler reducing learning rate to tf.Tensor(4.8800416e-06, shape=(), dtype=float32).\n",
      "Epoch 402/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0701 - nmse: 0.1741 - val_loss: 0.0724 - val_nmse: 0.1592\n",
      "\n",
      "Epoch 00403: LearningRateScheduler reducing learning rate to tf.Tensor(4.831484e-06, shape=(), dtype=float32).\n",
      "Epoch 403/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0701 - nmse: 0.1799 - val_loss: 0.0724 - val_nmse: 0.1592\n",
      "\n",
      "Epoch 00404: LearningRateScheduler reducing learning rate to tf.Tensor(4.7834096e-06, shape=(), dtype=float32).\n",
      "Epoch 404/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0701 - nmse: 0.1798 - val_loss: 0.0723 - val_nmse: 0.1592\n",
      "\n",
      "Epoch 00405: LearningRateScheduler reducing learning rate to tf.Tensor(4.7358135e-06, shape=(), dtype=float32).\n",
      "Epoch 405/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0701 - nmse: 0.1804 - val_loss: 0.0723 - val_nmse: 0.1592\n",
      "\n",
      "Epoch 00406: LearningRateScheduler reducing learning rate to tf.Tensor(4.688691e-06, shape=(), dtype=float32).\n",
      "Epoch 406/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0700 - nmse: 0.1751 - val_loss: 0.0723 - val_nmse: 0.1592\n",
      "\n",
      "Epoch 00407: LearningRateScheduler reducing learning rate to tf.Tensor(4.6420378e-06, shape=(), dtype=float32).\n",
      "Epoch 407/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0700 - nmse: 0.1781 - val_loss: 0.0723 - val_nmse: 0.1591\n",
      "\n",
      "Epoch 00408: LearningRateScheduler reducing learning rate to tf.Tensor(4.5958486e-06, shape=(), dtype=float32).\n",
      "Epoch 408/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0700 - nmse: 0.1772 - val_loss: 0.0723 - val_nmse: 0.1591\n",
      "\n",
      "Epoch 00409: LearningRateScheduler reducing learning rate to tf.Tensor(4.550119e-06, shape=(), dtype=float32).\n",
      "Epoch 409/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0700 - nmse: 0.1798 - val_loss: 0.0723 - val_nmse: 0.1591\n",
      "\n",
      "Epoch 00410: LearningRateScheduler reducing learning rate to tf.Tensor(4.504844e-06, shape=(), dtype=float32).\n",
      "Epoch 410/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0700 - nmse: 0.1823 - val_loss: 0.0723 - val_nmse: 0.1591\n",
      "\n",
      "Epoch 00411: LearningRateScheduler reducing learning rate to tf.Tensor(4.46002e-06, shape=(), dtype=float32).\n",
      "Epoch 411/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0700 - nmse: 0.1784 - val_loss: 0.0723 - val_nmse: 0.1591\n",
      "\n",
      "Epoch 00412: LearningRateScheduler reducing learning rate to tf.Tensor(4.415642e-06, shape=(), dtype=float32).\n",
      "Epoch 412/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0699 - nmse: 0.1793 - val_loss: 0.0723 - val_nmse: 0.1590\n",
      "\n",
      "Epoch 00413: LearningRateScheduler reducing learning rate to tf.Tensor(4.371705e-06, shape=(), dtype=float32).\n",
      "Epoch 413/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0699 - nmse: 0.1810 - val_loss: 0.0723 - val_nmse: 0.1590\n",
      "\n",
      "Epoch 00414: LearningRateScheduler reducing learning rate to tf.Tensor(4.3282057e-06, shape=(), dtype=float32).\n",
      "Epoch 414/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0699 - nmse: 0.1778 - val_loss: 0.0723 - val_nmse: 0.1590\n",
      "\n",
      "Epoch 00415: LearningRateScheduler reducing learning rate to tf.Tensor(4.2851393e-06, shape=(), dtype=float32).\n",
      "Epoch 415/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0699 - nmse: 0.1790 - val_loss: 0.0723 - val_nmse: 0.1590\n",
      "\n",
      "Epoch 00416: LearningRateScheduler reducing learning rate to tf.Tensor(4.2425013e-06, shape=(), dtype=float32).\n",
      "Epoch 416/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0698 - nmse: 0.1817 - val_loss: 0.0723 - val_nmse: 0.1589\n",
      "\n",
      "Epoch 00417: LearningRateScheduler reducing learning rate to tf.Tensor(4.2002875e-06, shape=(), dtype=float32).\n",
      "Epoch 417/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0698 - nmse: 0.1813 - val_loss: 0.0723 - val_nmse: 0.1589\n",
      "\n",
      "Epoch 00418: LearningRateScheduler reducing learning rate to tf.Tensor(4.1584935e-06, shape=(), dtype=float32).\n",
      "Epoch 418/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0698 - nmse: 0.1843 - val_loss: 0.0722 - val_nmse: 0.1589\n",
      "\n",
      "Epoch 00419: LearningRateScheduler reducing learning rate to tf.Tensor(4.1171156e-06, shape=(), dtype=float32).\n",
      "Epoch 419/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0698 - nmse: 0.1795 - val_loss: 0.0722 - val_nmse: 0.1589\n",
      "\n",
      "Epoch 00420: LearningRateScheduler reducing learning rate to tf.Tensor(4.0761493e-06, shape=(), dtype=float32).\n",
      "Epoch 420/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0698 - nmse: 0.1746 - val_loss: 0.0722 - val_nmse: 0.1589\n",
      "\n",
      "Epoch 00421: LearningRateScheduler reducing learning rate to tf.Tensor(4.035591e-06, shape=(), dtype=float32).\n",
      "Epoch 421/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0698 - nmse: 0.1758 - val_loss: 0.0722 - val_nmse: 0.1588\n",
      "\n",
      "Epoch 00422: LearningRateScheduler reducing learning rate to tf.Tensor(3.9954357e-06, shape=(), dtype=float32).\n",
      "Epoch 422/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0697 - nmse: 0.1794 - val_loss: 0.0722 - val_nmse: 0.1588\n",
      "\n",
      "Epoch 00423: LearningRateScheduler reducing learning rate to tf.Tensor(3.9556803e-06, shape=(), dtype=float32).\n",
      "Epoch 423/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0697 - nmse: 0.1777 - val_loss: 0.0722 - val_nmse: 0.1588\n",
      "\n",
      "Epoch 00424: LearningRateScheduler reducing learning rate to tf.Tensor(3.9163206e-06, shape=(), dtype=float32).\n",
      "Epoch 424/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0697 - nmse: 0.1744 - val_loss: 0.0722 - val_nmse: 0.1587\n",
      "\n",
      "Epoch 00425: LearningRateScheduler reducing learning rate to tf.Tensor(3.8773524e-06, shape=(), dtype=float32).\n",
      "Epoch 425/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0697 - nmse: 0.1770 - val_loss: 0.0722 - val_nmse: 0.1587\n",
      "\n",
      "Epoch 00426: LearningRateScheduler reducing learning rate to tf.Tensor(3.838772e-06, shape=(), dtype=float32).\n",
      "Epoch 426/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0697 - nmse: 0.1770 - val_loss: 0.0722 - val_nmse: 0.1587\n",
      "\n",
      "Epoch 00427: LearningRateScheduler reducing learning rate to tf.Tensor(3.8005753e-06, shape=(), dtype=float32).\n",
      "Epoch 427/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0697 - nmse: 0.1747 - val_loss: 0.0722 - val_nmse: 0.1587\n",
      "\n",
      "Epoch 00428: LearningRateScheduler reducing learning rate to tf.Tensor(3.7627588e-06, shape=(), dtype=float32).\n",
      "Epoch 428/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0696 - nmse: 0.1822 - val_loss: 0.0721 - val_nmse: 0.1586\n",
      "\n",
      "Epoch 00429: LearningRateScheduler reducing learning rate to tf.Tensor(3.7253185e-06, shape=(), dtype=float32).\n",
      "Epoch 429/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0696 - nmse: 0.1788 - val_loss: 0.0721 - val_nmse: 0.1586\n",
      "\n",
      "Epoch 00430: LearningRateScheduler reducing learning rate to tf.Tensor(3.6882507e-06, shape=(), dtype=float32).\n",
      "Epoch 430/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0696 - nmse: 0.1824 - val_loss: 0.0721 - val_nmse: 0.1586\n",
      "\n",
      "Epoch 00431: LearningRateScheduler reducing learning rate to tf.Tensor(3.6515517e-06, shape=(), dtype=float32).\n",
      "Epoch 431/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0696 - nmse: 0.1748 - val_loss: 0.0722 - val_nmse: 0.1586\n",
      "\n",
      "Epoch 00432: LearningRateScheduler reducing learning rate to tf.Tensor(3.615218e-06, shape=(), dtype=float32).\n",
      "Epoch 432/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0696 - nmse: 0.1797 - val_loss: 0.0722 - val_nmse: 0.1586\n",
      "\n",
      "Epoch 00433: LearningRateScheduler reducing learning rate to tf.Tensor(3.579246e-06, shape=(), dtype=float32).\n",
      "Epoch 433/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0696 - nmse: 0.1706 - val_loss: 0.0721 - val_nmse: 0.1585\n",
      "\n",
      "Epoch 00434: LearningRateScheduler reducing learning rate to tf.Tensor(3.5436317e-06, shape=(), dtype=float32).\n",
      "Epoch 434/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0695 - nmse: 0.1813 - val_loss: 0.0721 - val_nmse: 0.1585\n",
      "\n",
      "Epoch 00435: LearningRateScheduler reducing learning rate to tf.Tensor(3.5083717e-06, shape=(), dtype=float32).\n",
      "Epoch 435/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0695 - nmse: 0.1774 - val_loss: 0.0721 - val_nmse: 0.1585\n",
      "\n",
      "Epoch 00436: LearningRateScheduler reducing learning rate to tf.Tensor(3.4734626e-06, shape=(), dtype=float32).\n",
      "Epoch 436/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0695 - nmse: 0.1793 - val_loss: 0.0721 - val_nmse: 0.1585\n",
      "\n",
      "Epoch 00437: LearningRateScheduler reducing learning rate to tf.Tensor(3.4389009e-06, shape=(), dtype=float32).\n",
      "Epoch 437/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0695 - nmse: 0.1750 - val_loss: 0.0721 - val_nmse: 0.1585\n",
      "\n",
      "Epoch 00438: LearningRateScheduler reducing learning rate to tf.Tensor(3.404683e-06, shape=(), dtype=float32).\n",
      "Epoch 438/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0695 - nmse: 0.1755 - val_loss: 0.0721 - val_nmse: 0.1585\n",
      "\n",
      "Epoch 00439: LearningRateScheduler reducing learning rate to tf.Tensor(3.3708056e-06, shape=(), dtype=float32).\n",
      "Epoch 439/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0695 - nmse: 0.1705 - val_loss: 0.0721 - val_nmse: 0.1584\n",
      "\n",
      "Epoch 00440: LearningRateScheduler reducing learning rate to tf.Tensor(3.3372653e-06, shape=(), dtype=float32).\n",
      "Epoch 440/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0694 - nmse: 0.1787 - val_loss: 0.0721 - val_nmse: 0.1584\n",
      "\n",
      "Epoch 00441: LearningRateScheduler reducing learning rate to tf.Tensor(3.3040587e-06, shape=(), dtype=float32).\n",
      "Epoch 441/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0694 - nmse: 0.1806 - val_loss: 0.0721 - val_nmse: 0.1584\n",
      "\n",
      "Epoch 00442: LearningRateScheduler reducing learning rate to tf.Tensor(3.2711825e-06, shape=(), dtype=float32).\n",
      "Epoch 442/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0694 - nmse: 0.1788 - val_loss: 0.0721 - val_nmse: 0.1584\n",
      "\n",
      "Epoch 00443: LearningRateScheduler reducing learning rate to tf.Tensor(3.2386336e-06, shape=(), dtype=float32).\n",
      "Epoch 443/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0694 - nmse: 0.1725 - val_loss: 0.0721 - val_nmse: 0.1583\n",
      "\n",
      "Epoch 00444: LearningRateScheduler reducing learning rate to tf.Tensor(3.2064083e-06, shape=(), dtype=float32).\n",
      "Epoch 444/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0694 - nmse: 0.1829 - val_loss: 0.0720 - val_nmse: 0.1583\n",
      "\n",
      "Epoch 00445: LearningRateScheduler reducing learning rate to tf.Tensor(3.174504e-06, shape=(), dtype=float32).\n",
      "Epoch 445/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0694 - nmse: 0.1748 - val_loss: 0.0720 - val_nmse: 0.1583\n",
      "\n",
      "Epoch 00446: LearningRateScheduler reducing learning rate to tf.Tensor(3.142917e-06, shape=(), dtype=float32).\n",
      "Epoch 446/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0694 - nmse: 0.1779 - val_loss: 0.0720 - val_nmse: 0.1583\n",
      "\n",
      "Epoch 00447: LearningRateScheduler reducing learning rate to tf.Tensor(3.1116442e-06, shape=(), dtype=float32).\n",
      "Epoch 447/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0693 - nmse: 0.1808 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00448: LearningRateScheduler reducing learning rate to tf.Tensor(3.0806827e-06, shape=(), dtype=float32).\n",
      "Epoch 448/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0693 - nmse: 0.1770 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00449: LearningRateScheduler reducing learning rate to tf.Tensor(3.0500294e-06, shape=(), dtype=float32).\n",
      "Epoch 449/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0693 - nmse: 0.1745 - val_loss: 0.0721 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00450: LearningRateScheduler reducing learning rate to tf.Tensor(3.019681e-06, shape=(), dtype=float32).\n",
      "Epoch 450/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0693 - nmse: 0.1772 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00451: LearningRateScheduler reducing learning rate to tf.Tensor(2.9896344e-06, shape=(), dtype=float32).\n",
      "Epoch 451/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0693 - nmse: 0.1781 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00452: LearningRateScheduler reducing learning rate to tf.Tensor(2.9598868e-06, shape=(), dtype=float32).\n",
      "Epoch 452/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0693 - nmse: 0.1792 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00453: LearningRateScheduler reducing learning rate to tf.Tensor(2.9304354e-06, shape=(), dtype=float32).\n",
      "Epoch 453/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0693 - nmse: 0.1783 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00454: LearningRateScheduler reducing learning rate to tf.Tensor(2.901277e-06, shape=(), dtype=float32).\n",
      "Epoch 454/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0692 - nmse: 0.1808 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00455: LearningRateScheduler reducing learning rate to tf.Tensor(2.8724087e-06, shape=(), dtype=float32).\n",
      "Epoch 455/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0692 - nmse: 0.1795 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00456: LearningRateScheduler reducing learning rate to tf.Tensor(2.8438276e-06, shape=(), dtype=float32).\n",
      "Epoch 456/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0692 - nmse: 0.1792 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00457: LearningRateScheduler reducing learning rate to tf.Tensor(2.815531e-06, shape=(), dtype=float32).\n",
      "Epoch 457/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0692 - nmse: 0.1777 - val_loss: 0.0720 - val_nmse: 0.1582\n",
      "\n",
      "Epoch 00458: LearningRateScheduler reducing learning rate to tf.Tensor(2.7875158e-06, shape=(), dtype=float32).\n",
      "Epoch 458/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0692 - nmse: 0.1754 - val_loss: 0.0720 - val_nmse: 0.1581\n",
      "\n",
      "Epoch 00459: LearningRateScheduler reducing learning rate to tf.Tensor(2.7597794e-06, shape=(), dtype=float32).\n",
      "Epoch 459/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0692 - nmse: 0.1787 - val_loss: 0.0720 - val_nmse: 0.1581\n",
      "\n",
      "Epoch 00460: LearningRateScheduler reducing learning rate to tf.Tensor(2.732319e-06, shape=(), dtype=float32).\n",
      "Epoch 460/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0692 - nmse: 0.1771 - val_loss: 0.0720 - val_nmse: 0.1581\n",
      "\n",
      "Epoch 00461: LearningRateScheduler reducing learning rate to tf.Tensor(2.705132e-06, shape=(), dtype=float32).\n",
      "Epoch 461/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0692 - nmse: 0.1798 - val_loss: 0.0720 - val_nmse: 0.1581\n",
      "\n",
      "Epoch 00462: LearningRateScheduler reducing learning rate to tf.Tensor(2.6782152e-06, shape=(), dtype=float32).\n",
      "Epoch 462/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0691 - nmse: 0.1716 - val_loss: 0.0720 - val_nmse: 0.1581\n",
      "\n",
      "Epoch 00463: LearningRateScheduler reducing learning rate to tf.Tensor(2.6515663e-06, shape=(), dtype=float32).\n",
      "Epoch 463/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0691 - nmse: 0.1799 - val_loss: 0.0720 - val_nmse: 0.1581\n",
      "\n",
      "Epoch 00464: LearningRateScheduler reducing learning rate to tf.Tensor(2.6251826e-06, shape=(), dtype=float32).\n",
      "Epoch 464/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0691 - nmse: 0.1754 - val_loss: 0.0720 - val_nmse: 0.1580\n",
      "\n",
      "Epoch 00465: LearningRateScheduler reducing learning rate to tf.Tensor(2.5990614e-06, shape=(), dtype=float32).\n",
      "Epoch 465/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0691 - nmse: 0.1819 - val_loss: 0.0720 - val_nmse: 0.1580\n",
      "\n",
      "Epoch 00466: LearningRateScheduler reducing learning rate to tf.Tensor(2.5732002e-06, shape=(), dtype=float32).\n",
      "Epoch 466/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0691 - nmse: 0.1809 - val_loss: 0.0720 - val_nmse: 0.1580\n",
      "\n",
      "Epoch 00467: LearningRateScheduler reducing learning rate to tf.Tensor(2.5475963e-06, shape=(), dtype=float32).\n",
      "Epoch 467/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0691 - nmse: 0.1763 - val_loss: 0.0719 - val_nmse: 0.1580\n",
      "\n",
      "Epoch 00468: LearningRateScheduler reducing learning rate to tf.Tensor(2.522247e-06, shape=(), dtype=float32).\n",
      "Epoch 468/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0691 - nmse: 0.1793 - val_loss: 0.0719 - val_nmse: 0.1580\n",
      "\n",
      "Epoch 00469: LearningRateScheduler reducing learning rate to tf.Tensor(2.4971503e-06, shape=(), dtype=float32).\n",
      "Epoch 469/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0691 - nmse: 0.1772 - val_loss: 0.0719 - val_nmse: 0.1580\n",
      "\n",
      "Epoch 00470: LearningRateScheduler reducing learning rate to tf.Tensor(2.472303e-06, shape=(), dtype=float32).\n",
      "Epoch 470/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1741 - val_loss: 0.0719 - val_nmse: 0.1580\n",
      "\n",
      "Epoch 00471: LearningRateScheduler reducing learning rate to tf.Tensor(2.447703e-06, shape=(), dtype=float32).\n",
      "Epoch 471/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1772 - val_loss: 0.0719 - val_nmse: 0.1580\n",
      "\n",
      "Epoch 00472: LearningRateScheduler reducing learning rate to tf.Tensor(2.423348e-06, shape=(), dtype=float32).\n",
      "Epoch 472/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1765 - val_loss: 0.0719 - val_nmse: 0.1579\n",
      "\n",
      "Epoch 00473: LearningRateScheduler reducing learning rate to tf.Tensor(2.3992352e-06, shape=(), dtype=float32).\n",
      "Epoch 473/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1819 - val_loss: 0.0719 - val_nmse: 0.1579\n",
      "\n",
      "Epoch 00474: LearningRateScheduler reducing learning rate to tf.Tensor(2.3753623e-06, shape=(), dtype=float32).\n",
      "Epoch 474/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1807 - val_loss: 0.0719 - val_nmse: 0.1579\n",
      "\n",
      "Epoch 00475: LearningRateScheduler reducing learning rate to tf.Tensor(2.351727e-06, shape=(), dtype=float32).\n",
      "Epoch 475/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1717 - val_loss: 0.0719 - val_nmse: 0.1579\n",
      "\n",
      "Epoch 00476: LearningRateScheduler reducing learning rate to tf.Tensor(2.328327e-06, shape=(), dtype=float32).\n",
      "Epoch 476/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1764 - val_loss: 0.0719 - val_nmse: 0.1579\n",
      "\n",
      "Epoch 00477: LearningRateScheduler reducing learning rate to tf.Tensor(2.3051596e-06, shape=(), dtype=float32).\n",
      "Epoch 477/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1742 - val_loss: 0.0719 - val_nmse: 0.1579\n",
      "\n",
      "Epoch 00478: LearningRateScheduler reducing learning rate to tf.Tensor(2.2822228e-06, shape=(), dtype=float32).\n",
      "Epoch 478/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1709 - val_loss: 0.0719 - val_nmse: 0.1579\n",
      "\n",
      "Epoch 00479: LearningRateScheduler reducing learning rate to tf.Tensor(2.259514e-06, shape=(), dtype=float32).\n",
      "Epoch 479/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0690 - nmse: 0.1697 - val_loss: 0.0719 - val_nmse: 0.1578\n",
      "\n",
      "Epoch 00480: LearningRateScheduler reducing learning rate to tf.Tensor(2.2370314e-06, shape=(), dtype=float32).\n",
      "Epoch 480/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0689 - nmse: 0.1792 - val_loss: 0.0719 - val_nmse: 0.1578\n",
      "\n",
      "Epoch 00481: LearningRateScheduler reducing learning rate to tf.Tensor(2.2147724e-06, shape=(), dtype=float32).\n",
      "Epoch 481/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0689 - nmse: 0.1806 - val_loss: 0.0719 - val_nmse: 0.1578\n",
      "\n",
      "Epoch 00482: LearningRateScheduler reducing learning rate to tf.Tensor(2.1927349e-06, shape=(), dtype=float32).\n",
      "Epoch 482/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0689 - nmse: 0.1805 - val_loss: 0.0719 - val_nmse: 0.1578\n",
      "\n",
      "Epoch 00483: LearningRateScheduler reducing learning rate to tf.Tensor(2.1709168e-06, shape=(), dtype=float32).\n",
      "Epoch 483/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0689 - nmse: 0.1793 - val_loss: 0.0719 - val_nmse: 0.1578\n",
      "\n",
      "Epoch 00484: LearningRateScheduler reducing learning rate to tf.Tensor(2.1493156e-06, shape=(), dtype=float32).\n",
      "Epoch 484/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0689 - nmse: 0.1778 - val_loss: 0.0719 - val_nmse: 0.1578\n",
      "\n",
      "Epoch 00485: LearningRateScheduler reducing learning rate to tf.Tensor(2.1279295e-06, shape=(), dtype=float32).\n",
      "Epoch 485/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0689 - nmse: 0.1773 - val_loss: 0.0719 - val_nmse: 0.1578\n",
      "\n",
      "Epoch 00486: LearningRateScheduler reducing learning rate to tf.Tensor(2.1067563e-06, shape=(), dtype=float32).\n",
      "Epoch 486/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0689 - nmse: 0.1775 - val_loss: 0.0719 - val_nmse: 0.1578\n",
      "\n",
      "Epoch 00487: LearningRateScheduler reducing learning rate to tf.Tensor(2.0857935e-06, shape=(), dtype=float32).\n",
      "Epoch 487/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0689 - nmse: 0.1778 - val_loss: 0.0719 - val_nmse: 0.1577\n",
      "\n",
      "Epoch 00488: LearningRateScheduler reducing learning rate to tf.Tensor(2.0650393e-06, shape=(), dtype=float32).\n",
      "Epoch 488/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0689 - nmse: 0.1730 - val_loss: 0.0719 - val_nmse: 0.1577\n",
      "\n",
      "Epoch 00489: LearningRateScheduler reducing learning rate to tf.Tensor(2.0444918e-06, shape=(), dtype=float32).\n",
      "Epoch 489/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1751 - val_loss: 0.0719 - val_nmse: 0.1577\n",
      "\n",
      "Epoch 00490: LearningRateScheduler reducing learning rate to tf.Tensor(2.0241487e-06, shape=(), dtype=float32).\n",
      "Epoch 490/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1719 - val_loss: 0.0719 - val_nmse: 0.1577\n",
      "\n",
      "Epoch 00491: LearningRateScheduler reducing learning rate to tf.Tensor(2.004008e-06, shape=(), dtype=float32).\n",
      "Epoch 491/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1774 - val_loss: 0.0719 - val_nmse: 0.1577\n",
      "\n",
      "Epoch 00492: LearningRateScheduler reducing learning rate to tf.Tensor(1.9840677e-06, shape=(), dtype=float32).\n",
      "Epoch 492/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1783 - val_loss: 0.0718 - val_nmse: 0.1577\n",
      "\n",
      "Epoch 00493: LearningRateScheduler reducing learning rate to tf.Tensor(1.9643257e-06, shape=(), dtype=float32).\n",
      "Epoch 493/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1776 - val_loss: 0.0718 - val_nmse: 0.1577\n",
      "\n",
      "Epoch 00494: LearningRateScheduler reducing learning rate to tf.Tensor(1.9447803e-06, shape=(), dtype=float32).\n",
      "Epoch 494/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1754 - val_loss: 0.0718 - val_nmse: 0.1577\n",
      "\n",
      "Epoch 00495: LearningRateScheduler reducing learning rate to tf.Tensor(1.9254292e-06, shape=(), dtype=float32).\n",
      "Epoch 495/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1749 - val_loss: 0.0718 - val_nmse: 0.1576\n",
      "\n",
      "Epoch 00496: LearningRateScheduler reducing learning rate to tf.Tensor(1.9062708e-06, shape=(), dtype=float32).\n",
      "Epoch 496/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1733 - val_loss: 0.0718 - val_nmse: 0.1576\n",
      "\n",
      "Epoch 00497: LearningRateScheduler reducing learning rate to tf.Tensor(1.8873029e-06, shape=(), dtype=float32).\n",
      "Epoch 497/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1730 - val_loss: 0.0718 - val_nmse: 0.1576\n",
      "\n",
      "Epoch 00498: LearningRateScheduler reducing learning rate to tf.Tensor(1.8685238e-06, shape=(), dtype=float32).\n",
      "Epoch 498/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1736 - val_loss: 0.0718 - val_nmse: 0.1576\n",
      "\n",
      "Epoch 00499: LearningRateScheduler reducing learning rate to tf.Tensor(1.8499316e-06, shape=(), dtype=float32).\n",
      "Epoch 499/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1698 - val_loss: 0.0718 - val_nmse: 0.1576\n",
      "\n",
      "Epoch 00500: LearningRateScheduler reducing learning rate to tf.Tensor(1.8315243e-06, shape=(), dtype=float32).\n",
      "Epoch 500/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0688 - nmse: 0.1789 - val_loss: 0.0718 - val_nmse: 0.1576\n",
      "\n",
      "Epoch 00501: LearningRateScheduler reducing learning rate to tf.Tensor(1.8133002e-06, shape=(), dtype=float32).\n",
      "Epoch 501/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0687 - nmse: 0.1738 - val_loss: 0.0718 - val_nmse: 0.1576\n",
      "\n",
      "Epoch 00502: LearningRateScheduler reducing learning rate to tf.Tensor(1.7952574e-06, shape=(), dtype=float32).\n",
      "Epoch 502/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0687 - nmse: 0.1746 - val_loss: 0.0718 - val_nmse: 0.1576\n",
      "\n",
      "Epoch 00503: LearningRateScheduler reducing learning rate to tf.Tensor(1.7773942e-06, shape=(), dtype=float32).\n",
      "Epoch 503/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0687 - nmse: 0.1751 - val_loss: 0.0718 - val_nmse: 0.1575\n",
      "\n",
      "Epoch 00504: LearningRateScheduler reducing learning rate to tf.Tensor(1.7597088e-06, shape=(), dtype=float32).\n",
      "Epoch 504/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0687 - nmse: 0.1786 - val_loss: 0.0718 - val_nmse: 0.1575\n",
      "\n",
      "Epoch 00505: LearningRateScheduler reducing learning rate to tf.Tensor(1.7421993e-06, shape=(), dtype=float32).\n",
      "Epoch 505/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0687 - nmse: 0.1781 - val_loss: 0.0718 - val_nmse: 0.1575\n",
      "\n",
      "Epoch 00506: LearningRateScheduler reducing learning rate to tf.Tensor(1.724864e-06, shape=(), dtype=float32).\n",
      "Epoch 506/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0687 - nmse: 0.1752 - val_loss: 0.0718 - val_nmse: 0.1575\n",
      "\n",
      "Epoch 00507: LearningRateScheduler reducing learning rate to tf.Tensor(1.7077012e-06, shape=(), dtype=float32).\n",
      "Epoch 507/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0687 - nmse: 0.1750 - val_loss: 0.0718 - val_nmse: 0.1575\n",
      "\n",
      "Epoch 00508: LearningRateScheduler reducing learning rate to tf.Tensor(1.6907093e-06, shape=(), dtype=float32).\n",
      "Epoch 508/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0687 - nmse: 0.1742 - val_loss: 0.0718 - val_nmse: 0.1575\n",
      "\n",
      "Epoch 00509: LearningRateScheduler reducing learning rate to tf.Tensor(1.6738863e-06, shape=(), dtype=float32).\n",
      "Epoch 509/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0687 - nmse: 0.1778 - val_loss: 0.0718 - val_nmse: 0.1575\n",
      "\n",
      "Epoch 00510: LearningRateScheduler reducing learning rate to tf.Tensor(1.6572308e-06, shape=(), dtype=float32).\n",
      "Epoch 510/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0687 - nmse: 0.1729 - val_loss: 0.0718 - val_nmse: 0.1575\n",
      "\n",
      "Epoch 00511: LearningRateScheduler reducing learning rate to tf.Tensor(1.640741e-06, shape=(), dtype=float32).\n",
      "Epoch 511/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0687 - nmse: 0.1728 - val_loss: 0.0718 - val_nmse: 0.1575\n",
      "\n",
      "Epoch 00512: LearningRateScheduler reducing learning rate to tf.Tensor(1.6244152e-06, shape=(), dtype=float32).\n",
      "Epoch 512/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0687 - nmse: 0.1801 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00513: LearningRateScheduler reducing learning rate to tf.Tensor(1.6082519e-06, shape=(), dtype=float32).\n",
      "Epoch 513/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1740 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00514: LearningRateScheduler reducing learning rate to tf.Tensor(1.5922494e-06, shape=(), dtype=float32).\n",
      "Epoch 514/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1684 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00515: LearningRateScheduler reducing learning rate to tf.Tensor(1.5764061e-06, shape=(), dtype=float32).\n",
      "Epoch 515/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1817 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00516: LearningRateScheduler reducing learning rate to tf.Tensor(1.5607205e-06, shape=(), dtype=float32).\n",
      "Epoch 516/700\n",
      "21/21 [==============================] - 1s 52ms/sample - loss: 0.0686 - nmse: 0.1725 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00517: LearningRateScheduler reducing learning rate to tf.Tensor(1.545191e-06, shape=(), dtype=float32).\n",
      "Epoch 517/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1753 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00518: LearningRateScheduler reducing learning rate to tf.Tensor(1.529816e-06, shape=(), dtype=float32).\n",
      "Epoch 518/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1720 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00519: LearningRateScheduler reducing learning rate to tf.Tensor(1.514594e-06, shape=(), dtype=float32).\n",
      "Epoch 519/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1749 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00520: LearningRateScheduler reducing learning rate to tf.Tensor(1.4995235e-06, shape=(), dtype=float32).\n",
      "Epoch 520/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1748 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00521: LearningRateScheduler reducing learning rate to tf.Tensor(1.4846029e-06, shape=(), dtype=float32).\n",
      "Epoch 521/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1680 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00522: LearningRateScheduler reducing learning rate to tf.Tensor(1.4698307e-06, shape=(), dtype=float32).\n",
      "Epoch 522/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1762 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00523: LearningRateScheduler reducing learning rate to tf.Tensor(1.4552056e-06, shape=(), dtype=float32).\n",
      "Epoch 523/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1710 - val_loss: 0.0718 - val_nmse: 0.1574\n",
      "\n",
      "Epoch 00524: LearningRateScheduler reducing learning rate to tf.Tensor(1.440726e-06, shape=(), dtype=float32).\n",
      "Epoch 524/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0686 - nmse: 0.1809 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00525: LearningRateScheduler reducing learning rate to tf.Tensor(1.4263904e-06, shape=(), dtype=float32).\n",
      "Epoch 525/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0686 - nmse: 0.1751 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00526: LearningRateScheduler reducing learning rate to tf.Tensor(1.4121975e-06, shape=(), dtype=float32).\n",
      "Epoch 526/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1752 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00527: LearningRateScheduler reducing learning rate to tf.Tensor(1.3981459e-06, shape=(), dtype=float32).\n",
      "Epoch 527/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1785 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00528: LearningRateScheduler reducing learning rate to tf.Tensor(1.384234e-06, shape=(), dtype=float32).\n",
      "Epoch 528/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1664 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00529: LearningRateScheduler reducing learning rate to tf.Tensor(1.3704606e-06, shape=(), dtype=float32).\n",
      "Epoch 529/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1719 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00530: LearningRateScheduler reducing learning rate to tf.Tensor(1.3568242e-06, shape=(), dtype=float32).\n",
      "Epoch 530/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0685 - nmse: 0.1756 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00531: LearningRateScheduler reducing learning rate to tf.Tensor(1.3433236e-06, shape=(), dtype=float32).\n",
      "Epoch 531/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1756 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00532: LearningRateScheduler reducing learning rate to tf.Tensor(1.3299572e-06, shape=(), dtype=float32).\n",
      "Epoch 532/700\n",
      "21/21 [==============================] - 1s 51ms/sample - loss: 0.0685 - nmse: 0.1744 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00533: LearningRateScheduler reducing learning rate to tf.Tensor(1.3167238e-06, shape=(), dtype=float32).\n",
      "Epoch 533/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1766 - val_loss: 0.0717 - val_nmse: 0.1573\n",
      "\n",
      "Epoch 00534: LearningRateScheduler reducing learning rate to tf.Tensor(1.3036221e-06, shape=(), dtype=float32).\n",
      "Epoch 534/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1779 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00535: LearningRateScheduler reducing learning rate to tf.Tensor(1.2906507e-06, shape=(), dtype=float32).\n",
      "Epoch 535/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1798 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00536: LearningRateScheduler reducing learning rate to tf.Tensor(1.2778085e-06, shape=(), dtype=float32).\n",
      "Epoch 536/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1732 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00537: LearningRateScheduler reducing learning rate to tf.Tensor(1.2650939e-06, shape=(), dtype=float32).\n",
      "Epoch 537/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1746 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00538: LearningRateScheduler reducing learning rate to tf.Tensor(1.252506e-06, shape=(), dtype=float32).\n",
      "Epoch 538/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1756 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00539: LearningRateScheduler reducing learning rate to tf.Tensor(1.2400433e-06, shape=(), dtype=float32).\n",
      "Epoch 539/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1695 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00540: LearningRateScheduler reducing learning rate to tf.Tensor(1.2277046e-06, shape=(), dtype=float32).\n",
      "Epoch 540/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0685 - nmse: 0.1732 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00541: LearningRateScheduler reducing learning rate to tf.Tensor(1.2154887e-06, shape=(), dtype=float32).\n",
      "Epoch 541/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0685 - nmse: 0.1760 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00542: LearningRateScheduler reducing learning rate to tf.Tensor(1.2033944e-06, shape=(), dtype=float32).\n",
      "Epoch 542/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1748 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00543: LearningRateScheduler reducing learning rate to tf.Tensor(1.1914203e-06, shape=(), dtype=float32).\n",
      "Epoch 543/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1774 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00544: LearningRateScheduler reducing learning rate to tf.Tensor(1.1795654e-06, shape=(), dtype=float32).\n",
      "Epoch 544/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1740 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00545: LearningRateScheduler reducing learning rate to tf.Tensor(1.1678285e-06, shape=(), dtype=float32).\n",
      "Epoch 545/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1759 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00546: LearningRateScheduler reducing learning rate to tf.Tensor(1.1562083e-06, shape=(), dtype=float32).\n",
      "Epoch 546/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1800 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00547: LearningRateScheduler reducing learning rate to tf.Tensor(1.1447038e-06, shape=(), dtype=float32).\n",
      "Epoch 547/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1785 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00548: LearningRateScheduler reducing learning rate to tf.Tensor(1.1333137e-06, shape=(), dtype=float32).\n",
      "Epoch 548/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1736 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00549: LearningRateScheduler reducing learning rate to tf.Tensor(1.122037e-06, shape=(), dtype=float32).\n",
      "Epoch 549/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1727 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00550: LearningRateScheduler reducing learning rate to tf.Tensor(1.1108725e-06, shape=(), dtype=float32).\n",
      "Epoch 550/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1790 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00551: LearningRateScheduler reducing learning rate to tf.Tensor(1.0998191e-06, shape=(), dtype=float32).\n",
      "Epoch 551/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1746 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00552: LearningRateScheduler reducing learning rate to tf.Tensor(1.0888756e-06, shape=(), dtype=float32).\n",
      "Epoch 552/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1730 - val_loss: 0.0717 - val_nmse: 0.1572\n",
      "\n",
      "Epoch 00553: LearningRateScheduler reducing learning rate to tf.Tensor(1.078041e-06, shape=(), dtype=float32).\n",
      "Epoch 553/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1783 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00554: LearningRateScheduler reducing learning rate to tf.Tensor(1.0673143e-06, shape=(), dtype=float32).\n",
      "Epoch 554/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1730 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00555: LearningRateScheduler reducing learning rate to tf.Tensor(1.0566943e-06, shape=(), dtype=float32).\n",
      "Epoch 555/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1797 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00556: LearningRateScheduler reducing learning rate to tf.Tensor(1.0461799e-06, shape=(), dtype=float32).\n",
      "Epoch 556/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1751 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00557: LearningRateScheduler reducing learning rate to tf.Tensor(1.0357702e-06, shape=(), dtype=float32).\n",
      "Epoch 557/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1763 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00558: LearningRateScheduler reducing learning rate to tf.Tensor(1.025464e-06, shape=(), dtype=float32).\n",
      "Epoch 558/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1741 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00559: LearningRateScheduler reducing learning rate to tf.Tensor(1.0152604e-06, shape=(), dtype=float32).\n",
      "Epoch 559/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0684 - nmse: 0.1780 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00560: LearningRateScheduler reducing learning rate to tf.Tensor(1.0051583e-06, shape=(), dtype=float32).\n",
      "Epoch 560/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1761 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00561: LearningRateScheduler reducing learning rate to tf.Tensor(9.951567e-07, shape=(), dtype=float32).\n",
      "Epoch 561/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1765 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00562: LearningRateScheduler reducing learning rate to tf.Tensor(9.852547e-07, shape=(), dtype=float32).\n",
      "Epoch 562/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1733 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00563: LearningRateScheduler reducing learning rate to tf.Tensor(9.754511e-07, shape=(), dtype=float32).\n",
      "Epoch 563/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0683 - nmse: 0.1685 - val_loss: 0.0717 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00564: LearningRateScheduler reducing learning rate to tf.Tensor(9.657452e-07, shape=(), dtype=float32).\n",
      "Epoch 564/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0683 - nmse: 0.1723 - val_loss: 0.0716 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00565: LearningRateScheduler reducing learning rate to tf.Tensor(9.561359e-07, shape=(), dtype=float32).\n",
      "Epoch 565/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1734 - val_loss: 0.0716 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00566: LearningRateScheduler reducing learning rate to tf.Tensor(9.466221e-07, shape=(), dtype=float32).\n",
      "Epoch 566/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1759 - val_loss: 0.0716 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00567: LearningRateScheduler reducing learning rate to tf.Tensor(9.3720297e-07, shape=(), dtype=float32).\n",
      "Epoch 567/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0683 - nmse: 0.1724 - val_loss: 0.0716 - val_nmse: 0.1571\n",
      "\n",
      "Epoch 00568: LearningRateScheduler reducing learning rate to tf.Tensor(9.278776e-07, shape=(), dtype=float32).\n",
      "Epoch 568/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1720 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00569: LearningRateScheduler reducing learning rate to tf.Tensor(9.18645e-07, shape=(), dtype=float32).\n",
      "Epoch 569/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1704 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00570: LearningRateScheduler reducing learning rate to tf.Tensor(9.095043e-07, shape=(), dtype=float32).\n",
      "Epoch 570/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1802 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00571: LearningRateScheduler reducing learning rate to tf.Tensor(9.0045455e-07, shape=(), dtype=float32).\n",
      "Epoch 571/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1766 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00572: LearningRateScheduler reducing learning rate to tf.Tensor(8.9149484e-07, shape=(), dtype=float32).\n",
      "Epoch 572/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1733 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00573: LearningRateScheduler reducing learning rate to tf.Tensor(8.8262425e-07, shape=(), dtype=float32).\n",
      "Epoch 573/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1737 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00574: LearningRateScheduler reducing learning rate to tf.Tensor(8.7384194e-07, shape=(), dtype=float32).\n",
      "Epoch 574/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0683 - nmse: 0.1797 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00575: LearningRateScheduler reducing learning rate to tf.Tensor(8.65147e-07, shape=(), dtype=float32).\n",
      "Epoch 575/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1773 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00576: LearningRateScheduler reducing learning rate to tf.Tensor(8.565386e-07, shape=(), dtype=float32).\n",
      "Epoch 576/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1766 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00577: LearningRateScheduler reducing learning rate to tf.Tensor(8.480158e-07, shape=(), dtype=float32).\n",
      "Epoch 577/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1724 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00578: LearningRateScheduler reducing learning rate to tf.Tensor(8.3957787e-07, shape=(), dtype=float32).\n",
      "Epoch 578/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1783 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00579: LearningRateScheduler reducing learning rate to tf.Tensor(8.312239e-07, shape=(), dtype=float32).\n",
      "Epoch 579/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1737 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00580: LearningRateScheduler reducing learning rate to tf.Tensor(8.22953e-07, shape=(), dtype=float32).\n",
      "Epoch 580/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0683 - nmse: 0.1723 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00581: LearningRateScheduler reducing learning rate to tf.Tensor(8.147644e-07, shape=(), dtype=float32).\n",
      "Epoch 581/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0682 - nmse: 0.1707 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00582: LearningRateScheduler reducing learning rate to tf.Tensor(8.0665734e-07, shape=(), dtype=float32).\n",
      "Epoch 582/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0682 - nmse: 0.1720 - val_loss: 0.0716 - val_nmse: 0.1570\n",
      "\n",
      "Epoch 00583: LearningRateScheduler reducing learning rate to tf.Tensor(7.9863094e-07, shape=(), dtype=float32).\n",
      "Epoch 583/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1769 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00584: LearningRateScheduler reducing learning rate to tf.Tensor(7.906844e-07, shape=(), dtype=float32).\n",
      "Epoch 584/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1758 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00585: LearningRateScheduler reducing learning rate to tf.Tensor(7.828169e-07, shape=(), dtype=float32).\n",
      "Epoch 585/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1809 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00586: LearningRateScheduler reducing learning rate to tf.Tensor(7.7502773e-07, shape=(), dtype=float32).\n",
      "Epoch 586/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1768 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00587: LearningRateScheduler reducing learning rate to tf.Tensor(7.67316e-07, shape=(), dtype=float32).\n",
      "Epoch 587/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1758 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00588: LearningRateScheduler reducing learning rate to tf.Tensor(7.5968103e-07, shape=(), dtype=float32).\n",
      "Epoch 588/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1767 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00589: LearningRateScheduler reducing learning rate to tf.Tensor(7.5212205e-07, shape=(), dtype=float32).\n",
      "Epoch 589/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1749 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00590: LearningRateScheduler reducing learning rate to tf.Tensor(7.4463827e-07, shape=(), dtype=float32).\n",
      "Epoch 590/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1758 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00591: LearningRateScheduler reducing learning rate to tf.Tensor(7.3722896e-07, shape=(), dtype=float32).\n",
      "Epoch 591/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1711 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00592: LearningRateScheduler reducing learning rate to tf.Tensor(7.298934e-07, shape=(), dtype=float32).\n",
      "Epoch 592/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1707 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00593: LearningRateScheduler reducing learning rate to tf.Tensor(7.226308e-07, shape=(), dtype=float32).\n",
      "Epoch 593/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1771 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00594: LearningRateScheduler reducing learning rate to tf.Tensor(7.154404e-07, shape=(), dtype=float32).\n",
      "Epoch 594/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1763 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00595: LearningRateScheduler reducing learning rate to tf.Tensor(7.0832164e-07, shape=(), dtype=float32).\n",
      "Epoch 595/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1761 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00596: LearningRateScheduler reducing learning rate to tf.Tensor(7.012737e-07, shape=(), dtype=float32).\n",
      "Epoch 596/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1747 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00597: LearningRateScheduler reducing learning rate to tf.Tensor(6.9429586e-07, shape=(), dtype=float32).\n",
      "Epoch 597/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1694 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00598: LearningRateScheduler reducing learning rate to tf.Tensor(6.8738746e-07, shape=(), dtype=float32).\n",
      "Epoch 598/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1742 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00599: LearningRateScheduler reducing learning rate to tf.Tensor(6.8054777e-07, shape=(), dtype=float32).\n",
      "Epoch 599/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1725 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00600: LearningRateScheduler reducing learning rate to tf.Tensor(6.737762e-07, shape=(), dtype=float32).\n",
      "Epoch 600/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1733 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00601: LearningRateScheduler reducing learning rate to tf.Tensor(6.6707196e-07, shape=(), dtype=float32).\n",
      "Epoch 601/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1686 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00602: LearningRateScheduler reducing learning rate to tf.Tensor(6.6043447e-07, shape=(), dtype=float32).\n",
      "Epoch 602/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1781 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00603: LearningRateScheduler reducing learning rate to tf.Tensor(6.53863e-07, shape=(), dtype=float32).\n",
      "Epoch 603/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1777 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00604: LearningRateScheduler reducing learning rate to tf.Tensor(6.4735696e-07, shape=(), dtype=float32).\n",
      "Epoch 604/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1742 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00605: LearningRateScheduler reducing learning rate to tf.Tensor(6.4091563e-07, shape=(), dtype=float32).\n",
      "Epoch 605/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1769 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00606: LearningRateScheduler reducing learning rate to tf.Tensor(6.3453837e-07, shape=(), dtype=float32).\n",
      "Epoch 606/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1762 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00607: LearningRateScheduler reducing learning rate to tf.Tensor(6.282246e-07, shape=(), dtype=float32).\n",
      "Epoch 607/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0682 - nmse: 0.1723 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00608: LearningRateScheduler reducing learning rate to tf.Tensor(6.2197364e-07, shape=(), dtype=float32).\n",
      "Epoch 608/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1804 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00609: LearningRateScheduler reducing learning rate to tf.Tensor(6.1578487e-07, shape=(), dtype=float32).\n",
      "Epoch 609/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1784 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00610: LearningRateScheduler reducing learning rate to tf.Tensor(6.0965766e-07, shape=(), dtype=float32).\n",
      "Epoch 610/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1770 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00611: LearningRateScheduler reducing learning rate to tf.Tensor(6.0359145e-07, shape=(), dtype=float32).\n",
      "Epoch 611/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1788 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00612: LearningRateScheduler reducing learning rate to tf.Tensor(5.975856e-07, shape=(), dtype=float32).\n",
      "Epoch 612/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1763 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00613: LearningRateScheduler reducing learning rate to tf.Tensor(5.916395e-07, shape=(), dtype=float32).\n",
      "Epoch 613/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1729 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00614: LearningRateScheduler reducing learning rate to tf.Tensor(5.8575256e-07, shape=(), dtype=float32).\n",
      "Epoch 614/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1746 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00615: LearningRateScheduler reducing learning rate to tf.Tensor(5.799242e-07, shape=(), dtype=float32).\n",
      "Epoch 615/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1715 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00616: LearningRateScheduler reducing learning rate to tf.Tensor(5.7415383e-07, shape=(), dtype=float32).\n",
      "Epoch 616/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1713 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00617: LearningRateScheduler reducing learning rate to tf.Tensor(5.684409e-07, shape=(), dtype=float32).\n",
      "Epoch 617/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1716 - val_loss: 0.0716 - val_nmse: 0.1569\n",
      "\n",
      "Epoch 00618: LearningRateScheduler reducing learning rate to tf.Tensor(5.627848e-07, shape=(), dtype=float32).\n",
      "Epoch 618/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1713 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00619: LearningRateScheduler reducing learning rate to tf.Tensor(5.57185e-07, shape=(), dtype=float32).\n",
      "Epoch 619/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0681 - nmse: 0.1702 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00620: LearningRateScheduler reducing learning rate to tf.Tensor(5.516409e-07, shape=(), dtype=float32).\n",
      "Epoch 620/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1660 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00621: LearningRateScheduler reducing learning rate to tf.Tensor(5.461519e-07, shape=(), dtype=float32).\n",
      "Epoch 621/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1790 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00622: LearningRateScheduler reducing learning rate to tf.Tensor(5.4071756e-07, shape=(), dtype=float32).\n",
      "Epoch 622/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1755 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00623: LearningRateScheduler reducing learning rate to tf.Tensor(5.353373e-07, shape=(), dtype=float32).\n",
      "Epoch 623/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0681 - nmse: 0.1700 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00624: LearningRateScheduler reducing learning rate to tf.Tensor(5.3001054e-07, shape=(), dtype=float32).\n",
      "Epoch 624/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1772 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00625: LearningRateScheduler reducing learning rate to tf.Tensor(5.2473683e-07, shape=(), dtype=float32).\n",
      "Epoch 625/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1759 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00626: LearningRateScheduler reducing learning rate to tf.Tensor(5.195156e-07, shape=(), dtype=float32).\n",
      "Epoch 626/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1699 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00627: LearningRateScheduler reducing learning rate to tf.Tensor(5.143463e-07, shape=(), dtype=float32).\n",
      "Epoch 627/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1772 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00628: LearningRateScheduler reducing learning rate to tf.Tensor(5.0922847e-07, shape=(), dtype=float32).\n",
      "Epoch 628/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1776 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00629: LearningRateScheduler reducing learning rate to tf.Tensor(5.0416156e-07, shape=(), dtype=float32).\n",
      "Epoch 629/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1774 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00630: LearningRateScheduler reducing learning rate to tf.Tensor(4.99145e-07, shape=(), dtype=float32).\n",
      "Epoch 630/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1738 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00631: LearningRateScheduler reducing learning rate to tf.Tensor(4.9417844e-07, shape=(), dtype=float32).\n",
      "Epoch 631/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1753 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00632: LearningRateScheduler reducing learning rate to tf.Tensor(4.8926125e-07, shape=(), dtype=float32).\n",
      "Epoch 632/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1767 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00633: LearningRateScheduler reducing learning rate to tf.Tensor(4.84393e-07, shape=(), dtype=float32).\n",
      "Epoch 633/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1747 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00634: LearningRateScheduler reducing learning rate to tf.Tensor(4.795732e-07, shape=(), dtype=float32).\n",
      "Epoch 634/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1731 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00635: LearningRateScheduler reducing learning rate to tf.Tensor(4.7480134e-07, shape=(), dtype=float32).\n",
      "Epoch 635/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1755 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00636: LearningRateScheduler reducing learning rate to tf.Tensor(4.7007697e-07, shape=(), dtype=float32).\n",
      "Epoch 636/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1750 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00637: LearningRateScheduler reducing learning rate to tf.Tensor(4.653996e-07, shape=(), dtype=float32).\n",
      "Epoch 637/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1764 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00638: LearningRateScheduler reducing learning rate to tf.Tensor(4.6076877e-07, shape=(), dtype=float32).\n",
      "Epoch 638/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1759 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00639: LearningRateScheduler reducing learning rate to tf.Tensor(4.56184e-07, shape=(), dtype=float32).\n",
      "Epoch 639/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1781 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00640: LearningRateScheduler reducing learning rate to tf.Tensor(4.516449e-07, shape=(), dtype=float32).\n",
      "Epoch 640/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1737 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00641: LearningRateScheduler reducing learning rate to tf.Tensor(4.4715094e-07, shape=(), dtype=float32).\n",
      "Epoch 641/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1699 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00642: LearningRateScheduler reducing learning rate to tf.Tensor(4.427017e-07, shape=(), dtype=float32).\n",
      "Epoch 642/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0681 - nmse: 0.1745 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00643: LearningRateScheduler reducing learning rate to tf.Tensor(4.3829672e-07, shape=(), dtype=float32).\n",
      "Epoch 643/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1783 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00644: LearningRateScheduler reducing learning rate to tf.Tensor(4.3393558e-07, shape=(), dtype=float32).\n",
      "Epoch 644/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1741 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00645: LearningRateScheduler reducing learning rate to tf.Tensor(4.2961784e-07, shape=(), dtype=float32).\n",
      "Epoch 645/700\n",
      "21/21 [==============================] - 1s 51ms/sample - loss: 0.0680 - nmse: 0.1791 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00646: LearningRateScheduler reducing learning rate to tf.Tensor(4.2534305e-07, shape=(), dtype=float32).\n",
      "Epoch 646/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1763 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00647: LearningRateScheduler reducing learning rate to tf.Tensor(4.211108e-07, shape=(), dtype=float32).\n",
      "Epoch 647/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1716 - val_loss: 0.0716 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00648: LearningRateScheduler reducing learning rate to tf.Tensor(4.1692064e-07, shape=(), dtype=float32).\n",
      "Epoch 648/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1791 - val_loss: 0.0715 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00649: LearningRateScheduler reducing learning rate to tf.Tensor(4.1277218e-07, shape=(), dtype=float32).\n",
      "Epoch 649/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1723 - val_loss: 0.0715 - val_nmse: 0.1568\n",
      "\n",
      "Epoch 00650: LearningRateScheduler reducing learning rate to tf.Tensor(4.0866502e-07, shape=(), dtype=float32).\n",
      "Epoch 650/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0680 - nmse: 0.1725 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00651: LearningRateScheduler reducing learning rate to tf.Tensor(4.0459872e-07, shape=(), dtype=float32).\n",
      "Epoch 651/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1772 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00652: LearningRateScheduler reducing learning rate to tf.Tensor(4.0057287e-07, shape=(), dtype=float32).\n",
      "Epoch 652/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1743 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00653: LearningRateScheduler reducing learning rate to tf.Tensor(3.965871e-07, shape=(), dtype=float32).\n",
      "Epoch 653/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1675 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00654: LearningRateScheduler reducing learning rate to tf.Tensor(3.9264097e-07, shape=(), dtype=float32).\n",
      "Epoch 654/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1744 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00655: LearningRateScheduler reducing learning rate to tf.Tensor(3.887341e-07, shape=(), dtype=float32).\n",
      "Epoch 655/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1728 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00656: LearningRateScheduler reducing learning rate to tf.Tensor(3.848661e-07, shape=(), dtype=float32).\n",
      "Epoch 656/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1727 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00657: LearningRateScheduler reducing learning rate to tf.Tensor(3.8103659e-07, shape=(), dtype=float32).\n",
      "Epoch 657/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1703 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00658: LearningRateScheduler reducing learning rate to tf.Tensor(3.772452e-07, shape=(), dtype=float32).\n",
      "Epoch 658/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1761 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00659: LearningRateScheduler reducing learning rate to tf.Tensor(3.734915e-07, shape=(), dtype=float32).\n",
      "Epoch 659/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1753 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00660: LearningRateScheduler reducing learning rate to tf.Tensor(3.697752e-07, shape=(), dtype=float32).\n",
      "Epoch 660/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1768 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00661: LearningRateScheduler reducing learning rate to tf.Tensor(3.6609586e-07, shape=(), dtype=float32).\n",
      "Epoch 661/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1772 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00662: LearningRateScheduler reducing learning rate to tf.Tensor(3.6245314e-07, shape=(), dtype=float32).\n",
      "Epoch 662/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1750 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00663: LearningRateScheduler reducing learning rate to tf.Tensor(3.5884665e-07, shape=(), dtype=float32).\n",
      "Epoch 663/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1736 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00664: LearningRateScheduler reducing learning rate to tf.Tensor(3.5527606e-07, shape=(), dtype=float32).\n",
      "Epoch 664/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1744 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00665: LearningRateScheduler reducing learning rate to tf.Tensor(3.51741e-07, shape=(), dtype=float32).\n",
      "Epoch 665/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1773 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00666: LearningRateScheduler reducing learning rate to tf.Tensor(3.482411e-07, shape=(), dtype=float32).\n",
      "Epoch 666/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1727 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00667: LearningRateScheduler reducing learning rate to tf.Tensor(3.44776e-07, shape=(), dtype=float32).\n",
      "Epoch 667/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1769 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00668: LearningRateScheduler reducing learning rate to tf.Tensor(3.413454e-07, shape=(), dtype=float32).\n",
      "Epoch 668/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1733 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00669: LearningRateScheduler reducing learning rate to tf.Tensor(3.3794896e-07, shape=(), dtype=float32).\n",
      "Epoch 669/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1768 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00670: LearningRateScheduler reducing learning rate to tf.Tensor(3.345863e-07, shape=(), dtype=float32).\n",
      "Epoch 670/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1751 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00671: LearningRateScheduler reducing learning rate to tf.Tensor(3.312571e-07, shape=(), dtype=float32).\n",
      "Epoch 671/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1759 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00672: LearningRateScheduler reducing learning rate to tf.Tensor(3.27961e-07, shape=(), dtype=float32).\n",
      "Epoch 672/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1738 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00673: LearningRateScheduler reducing learning rate to tf.Tensor(3.246977e-07, shape=(), dtype=float32).\n",
      "Epoch 673/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1742 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00674: LearningRateScheduler reducing learning rate to tf.Tensor(3.214669e-07, shape=(), dtype=float32).\n",
      "Epoch 674/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1768 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00675: LearningRateScheduler reducing learning rate to tf.Tensor(3.1826823e-07, shape=(), dtype=float32).\n",
      "Epoch 675/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1775 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00676: LearningRateScheduler reducing learning rate to tf.Tensor(3.151014e-07, shape=(), dtype=float32).\n",
      "Epoch 676/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1752 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00677: LearningRateScheduler reducing learning rate to tf.Tensor(3.1196606e-07, shape=(), dtype=float32).\n",
      "Epoch 677/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0680 - nmse: 0.1778 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00678: LearningRateScheduler reducing learning rate to tf.Tensor(3.0886193e-07, shape=(), dtype=float32).\n",
      "Epoch 678/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1735 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00679: LearningRateScheduler reducing learning rate to tf.Tensor(3.057887e-07, shape=(), dtype=float32).\n",
      "Epoch 679/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1706 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00680: LearningRateScheduler reducing learning rate to tf.Tensor(3.0274603e-07, shape=(), dtype=float32).\n",
      "Epoch 680/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1744 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00681: LearningRateScheduler reducing learning rate to tf.Tensor(2.9973364e-07, shape=(), dtype=float32).\n",
      "Epoch 681/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1755 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00682: LearningRateScheduler reducing learning rate to tf.Tensor(2.9675124e-07, shape=(), dtype=float32).\n",
      "Epoch 682/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1750 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00683: LearningRateScheduler reducing learning rate to tf.Tensor(2.937985e-07, shape=(), dtype=float32).\n",
      "Epoch 683/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1681 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00684: LearningRateScheduler reducing learning rate to tf.Tensor(2.9087514e-07, shape=(), dtype=float32).\n",
      "Epoch 684/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0680 - nmse: 0.1737 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00685: LearningRateScheduler reducing learning rate to tf.Tensor(2.8798087e-07, shape=(), dtype=float32).\n",
      "Epoch 685/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1721 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00686: LearningRateScheduler reducing learning rate to tf.Tensor(2.851154e-07, shape=(), dtype=float32).\n",
      "Epoch 686/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1740 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00687: LearningRateScheduler reducing learning rate to tf.Tensor(2.8227842e-07, shape=(), dtype=float32).\n",
      "Epoch 687/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1692 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00688: LearningRateScheduler reducing learning rate to tf.Tensor(2.794697e-07, shape=(), dtype=float32).\n",
      "Epoch 688/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1694 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00689: LearningRateScheduler reducing learning rate to tf.Tensor(2.7668892e-07, shape=(), dtype=float32).\n",
      "Epoch 689/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1768 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00690: LearningRateScheduler reducing learning rate to tf.Tensor(2.739358e-07, shape=(), dtype=float32).\n",
      "Epoch 690/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1720 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00691: LearningRateScheduler reducing learning rate to tf.Tensor(2.7121007e-07, shape=(), dtype=float32).\n",
      "Epoch 691/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1686 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00692: LearningRateScheduler reducing learning rate to tf.Tensor(2.6851146e-07, shape=(), dtype=float32).\n",
      "Epoch 692/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1694 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00693: LearningRateScheduler reducing learning rate to tf.Tensor(2.658397e-07, shape=(), dtype=float32).\n",
      "Epoch 693/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0680 - nmse: 0.1725 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00694: LearningRateScheduler reducing learning rate to tf.Tensor(2.6319455e-07, shape=(), dtype=float32).\n",
      "Epoch 694/700\n",
      "21/21 [==============================] - 1s 49ms/sample - loss: 0.0679 - nmse: 0.1744 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00695: LearningRateScheduler reducing learning rate to tf.Tensor(2.6057572e-07, shape=(), dtype=float32).\n",
      "Epoch 695/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0679 - nmse: 0.1771 - val_loss: 0.0715 - val_nmse: 0.1567\n",
      "\n",
      "Epoch 00696: LearningRateScheduler reducing learning rate to tf.Tensor(2.5798292e-07, shape=(), dtype=float32).\n",
      "Epoch 696/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0679 - nmse: 0.1731 - val_loss: 0.0715 - val_nmse: 0.1566\n",
      "\n",
      "Epoch 00697: LearningRateScheduler reducing learning rate to tf.Tensor(2.5541593e-07, shape=(), dtype=float32).\n",
      "Epoch 697/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0679 - nmse: 0.1739 - val_loss: 0.0715 - val_nmse: 0.1566\n",
      "\n",
      "Epoch 00698: LearningRateScheduler reducing learning rate to tf.Tensor(2.528745e-07, shape=(), dtype=float32).\n",
      "Epoch 698/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0679 - nmse: 0.1752 - val_loss: 0.0715 - val_nmse: 0.1566\n",
      "\n",
      "Epoch 00699: LearningRateScheduler reducing learning rate to tf.Tensor(2.5035834e-07, shape=(), dtype=float32).\n",
      "Epoch 699/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0679 - nmse: 0.1691 - val_loss: 0.0715 - val_nmse: 0.1566\n",
      "\n",
      "Epoch 00700: LearningRateScheduler reducing learning rate to tf.Tensor(2.478672e-07, shape=(), dtype=float32).\n",
      "Epoch 700/700\n",
      "21/21 [==============================] - 1s 50ms/sample - loss: 0.0679 - nmse: 0.1765 - val_loss: 0.0715 - val_nmse: 0.1566\n",
      "Tiempo total de entrenamiento: 12.35 minutos.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history = model.fit(X, Y, epochs=max_epoch, batch_size=batchsize, callbacks=[callback], validation_split=validation_split)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "min_time = total_time / 60\n",
    "print(f'Tiempo total de entrenamiento: {min_time:.2f} minutos.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################################################################<br>\n",
    "#########################################################################################################<br>\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar el NMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['nmse'], label='NMSE (entrenamiento)')\n",
    "plt.plot(history.history['val_nmse'], label='NMSE (validacion)')\n",
    "plt.xlabel('Epoca')\n",
    "plt.ylabel('NMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myred",
   "language": "python",
   "name": "myred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
